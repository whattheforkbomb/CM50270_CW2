{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "v0mrHhpYZzxP"
   },
   "source": [
    "# AI-powered Mario\n",
    "\n",
    "Authors: [Yuansong Feng](https://github.com/YuansongFeng), [Suraj Subramanian](https://github.com/suraj813), [Howard Wang](https://github.com/hw26), [Steven Guo](https://github.com/GuoYuzhang).  \n",
    "\n",
    "\n",
    "## Welcome!\n",
    "This tutorial walks you through the fundamentals of Deep Reinforcement Learning. At the end, you will implement an AI-powered Mario (using [Double Deep Q-Networks](https://arxiv.org/pdf/1509.06461.pdf)) that can play the game by itself. \n",
    "\n",
    "Although no prior knowledge of RL is necessary for this tutorial, you can familiarize yourself with these RL [concepts](https://spinningup.openai.com/en/latest/spinningup/rl_intro.html), and have this handy [cheatsheet](https://colab.research.google.com/drive/1eN33dPVtdPViiS1njTW_-r-IYCDTFU7N) as your companion. The full code is available [here](https://github.com/yuansongFeng/MadMario/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SNstQbV0__cX"
   },
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 459
    },
    "executionInfo": {
     "elapsed": 15774,
     "status": "ok",
     "timestamp": 1603853036176,
     "user": {
      "displayName": "Yuansong Feng",
      "photoUrl": "",
      "userId": "12925789853286192206"
     },
     "user_tz": 420
    },
    "id": "_1NdXf43AUfY",
    "outputId": "ae4a2b61-0f4f-4b47-9a6e-27a2dcbe9cdd"
   },
   "outputs": [],
   "source": [
    "# Mario game environment\n",
    "#!pip install gym-super-mario-bros==7.3.0 opencv-python\n",
    "\n",
    "import os\n",
    "import copy\n",
    "import torch\n",
    "from torch import nn\n",
    "from pathlib import Path\n",
    "from collections import deque\n",
    "import random, datetime, numpy as np, cv2 \n",
    "# Gym is an OpenAI toolkit for RL\n",
    "import gym\n",
    "from gym.spaces import Box\n",
    "from gym.wrappers import FrameStack, GrayScaleObservation, TransformObservation\n",
    "\n",
    "#NES Emulator for OpenAI Gym\n",
    "from nes_py.wrappers import JoypadSpace\n",
    "\n",
    "# Super Mario environment for OpenAI Gym\n",
    "import gym_super_mario_bros"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ntFndnb7Co-E"
   },
   "source": [
    "## RL Definitions\n",
    "\n",
    "**Environment** \n",
    "The world that an agent interacts with and learns from.\n",
    "\n",
    "**Action** $a$ : \n",
    "How the Agent responds to the Environment. The set of all possible Actions is called *action-space*.\n",
    "\n",
    "**State** $s$ :\n",
    "The current characteristic of the Environment. The set of all possible States the Environment can be in is called *state-space*.\n",
    "\n",
    "**Reward** $r$ :\n",
    "Reward is the key feedback from Environment to Agent. It is what drives the Agent to learn and to change its future action. An aggregation of rewards over multiple time steps is called **Return**.\n",
    " \n",
    " **Optimal Action-Value function** $Q^*(s,a)$ :\n",
    "Gives the expected return if you start in state $s$, take an arbitrary action $a$, and then for each future time step take the action that maximizes returns. $Q$ can be said to stand for the \"quality\" of the action in a state. We try to approximate this function. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KdZlSOeNAyic"
   },
   "source": [
    "# Initialize Environment \n",
    "In Mario, the environment consists of tubes, mushrooms and other components. \n",
    "\n",
    "When Mario makes an action, the environment responds with the changed (next) state, reward and other info."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 85
    },
    "executionInfo": {
     "elapsed": 628,
     "status": "ok",
     "timestamp": 1603853043786,
     "user": {
      "displayName": "Yuansong Feng",
      "photoUrl": "",
      "userId": "12925789853286192206"
     },
     "user_tz": 420
    },
    "id": "6T6Ju170A0nB",
    "outputId": "b2af7e1e-1aaa-4122-bf17-9f9d977eebcd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(240, 256, 3),\n",
      " 0,\n",
      " False,\n",
      " {'coins': 0, 'flag_get': False, 'life': 2, 'score': 0, 'stage': 1, 'status': 'small', 'time': 400, 'world': 1, 'x_pos': 40, 'x_pos_screen': 40, 'y_pos': 79}\n"
     ]
    }
   ],
   "source": [
    "# Initialize Super Mario environment\n",
    "env = gym_super_mario_bros.make('SuperMarioBros-v3')\n",
    "\n",
    "# Limit the action-space to\n",
    "#   0. walk right \n",
    "#   1. jump right\n",
    "env = JoypadSpace(\n",
    "    env,\n",
    "    [['right'],\n",
    "    ['right', 'A']]\n",
    ")\n",
    "\n",
    "env.reset()\n",
    "next_state, reward, done, info = env.step(action=0)\n",
    "print(f'{next_state.shape},\\n {reward},\\n {done},\\n {info}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WZLgGn1iBXL8"
   },
   "source": [
    "# Preprocess Environment\n",
    "Environment data is returned to the agent in `next_state`. As you saw above, each state is represented by a `[3, 240, 256]` size array. Often that is more information than our agent needs; for instance, Mario's actions do not depend on the color of the pipes or the sky!\n",
    "\n",
    "We use **Wrappers** to preprocess environment data before sending it to the agent. \n",
    "\n",
    "`GrayScaleObservation` is a common wrapper to transform an RGB image to grayscale; doing so reduces the size of the state representation without losing useful information. Now the size of each state: `[1, 240, 256]`\n",
    "\n",
    "`ResizeObservation` downsamples each observation into a square image. New size: `[1, 84, 84]`\n",
    "\n",
    "`SkipFrame` is a custom wrapper that inherits from `gym.Wrapper` and implements the `step()` function. Because consecutive frames don't vary much, we can skip n-intermediate frames without losing much information. The n-th frame aggregates rewards accumulated over each skipped frame.\n",
    "\n",
    "`FrameStack` is a wrapper that allows us to squash consecutive frames of the environment into a single observation point to feed to our learning model. This way, we can identify if Mario was landing or jumping based on the direction of his movement in the previous several frames.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "GaKjHlNfGNMC"
   },
   "outputs": [],
   "source": [
    "class ResizeObservation(gym.ObservationWrapper):\n",
    "    def __init__(self, env, shape):\n",
    "        super().__init__(env)\n",
    "        if isinstance(shape, int):\n",
    "            self.shape = (shape, shape)\n",
    "        else:\n",
    "            self.shape = tuple(shape)\n",
    "\n",
    "        obs_shape = self.shape + self.observation_space.shape[2:]\n",
    "        self.observation_space = Box(low=0, high=255, shape=obs_shape, dtype=np.uint8)\n",
    "\n",
    "    def observation(self, observation):\n",
    "        observation = cv2.resize(observation, self.shape, interpolation=cv2.INTER_AREA)\n",
    "        return observation\n",
    "\n",
    "\n",
    "class SkipFrame(gym.Wrapper):\n",
    "    def __init__(self, env, skip):\n",
    "        \"\"\"Return only every `skip`-th frame\"\"\"\n",
    "        super().__init__(env)\n",
    "        self._skip = skip\n",
    "\n",
    "    def step(self, action):\n",
    "        \"\"\"Repeat action, and sum reward\"\"\"\n",
    "        total_reward = 0.0\n",
    "        done = False\n",
    "        for i in range(self._skip):\n",
    "            # Accumulate reward and repeat the same action\n",
    "            obs, reward, done, info = self.env.step(action)\n",
    "            total_reward += reward\n",
    "            if done:\n",
    "                break\n",
    "        return obs, total_reward, done, info\n",
    "\n",
    "\n",
    "# Apply Wrappers to environment\n",
    "env = SkipFrame(env, skip=4)\n",
    "env = GrayScaleObservation(env, keep_dim=False)\n",
    "env = ResizeObservation(env, shape=84)\n",
    "env = TransformObservation(env, f=lambda x: x / 255.)\n",
    "env = FrameStack(env, num_stack=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xIICJc3pNtiO"
   },
   "source": [
    "After applying the above wrappers to the environment, the final wrapped state consists of 4 gray-scaled consecutive frames stacked together, as shown above in the image on the left. Each time Mario makes an action, the environment responds with a state of this structure. The structure is represented by a 3-D array of size `[4, 84, 84]`.\n",
    "\n",
    "\n",
    "![picture](https://drive.google.com/uc?id=1zZU63qsuOKZIOwWt94z6cegOF2SMEmvD)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6ajnkJJgPOPH"
   },
   "source": [
    "# Agent\n",
    "We create a class `Mario` to represent our agent in the game. Mario should be able to:\n",
    "\n",
    "- **Act** according to the optimal action policy based on the current state (of the environment).\n",
    "\n",
    "- **Remember** experiences. Experience = (current state, current action, reward, next state). Mario *caches* and later *recalls* his experiences to update his action policy.\n",
    "\n",
    "- **Learn** a better action policy over time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "gu_N7QFwUllG"
   },
   "outputs": [],
   "source": [
    "class Mario:\n",
    "    def __init__():\n",
    "        pass\n",
    "\n",
    "    def act(self, state):\n",
    "        \"\"\"Given a state, choose an epsilon-greedy action\"\"\"\n",
    "        pass\n",
    "\n",
    "    def cache(self, experience):\n",
    "        \"\"\"Add the experience to memory\"\"\"\n",
    "        pass\n",
    "\n",
    "    def recall(self):\n",
    "        \"\"\"Sample experiences from memory\"\"\"\n",
    "        pass\n",
    "\n",
    "    def learn(self):\n",
    "        \"\"\"Update online action value (Q) function with a batch of experiences\"\"\"\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RAaKC6YsWASz"
   },
   "source": [
    "In the following sections, we will populate Mario's parameters and define his functions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lap2V2KEK3hq"
   },
   "source": [
    "# Act\n",
    "\n",
    "For any given state, an agent can choose to do the most optimal action (***exploit***) or a random action (***explore***). \n",
    "\n",
    "Mario randomly explores with a chance of `self.exploration_rate`; when he chooses to exploit, he relies on `MarioNet` (implemented in `Learn` section) to provide the most optimal action."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "LCZGt9d-MGX4"
   },
   "outputs": [],
   "source": [
    "class Mario: \n",
    "  def __init__(self, state_dim, action_dim, save_dir):\n",
    "    self.state_dim = state_dim\n",
    "    self.action_dim = action_dim\n",
    "    self.save_dir = save_dir\n",
    "\n",
    "    self.use_cuda = torch.cuda.is_available()\n",
    "    \n",
    "    # Mario's DNN to predict the most optimal action - we implement this in the Learn section\n",
    "    self.net = MarioNet(self.state_dim, self.action_dim).float()\n",
    "    if self.use_cuda:\n",
    "      self.net = self.net.to(device='cuda')\n",
    "\n",
    "    self.init_exploration_rate = 0.7\n",
    "    self.exploration_rate = 0.7\n",
    "    self.exploration_rate_decay = 0.999999\n",
    "    self.exploration_rate_min = 0.1\n",
    "    self.curr_step = 0\n",
    "\n",
    "    self.save_every = 5e5   # no. of experiences between saving Mario Net\n",
    "    \n",
    "\n",
    "  def act(self, state):\n",
    "    \"\"\"\n",
    "    Given a state, choose an epsilon-greedy action and update value of step.\n",
    "\n",
    "    Inputs:\n",
    "    state(LazyFrame): A single observation of the current state, dimension is (state_dim)\n",
    "    Outputs:\n",
    "    action_idx (int): An integer representing which action Mario will perform\n",
    "    \"\"\"\n",
    "    # EXPLORE\n",
    "    if np.random.rand() < self.exploration_rate:\n",
    "        action_idx = np.random.randint(self.action_dim)\n",
    "\n",
    "    # EXPLOIT\n",
    "    else:\n",
    "        state = np.array(state)\n",
    "        state = torch.FloatTensor(state).cuda() if self.use_cuda else torch.FloatTensor(state)\n",
    "        state = state.unsqueeze(0)\n",
    "        action_values = self.net(state, model='online')[0]\n",
    "        action_idx = torch.argmax(action_values, axis=1).item()\n",
    "\n",
    "    # decrease exploration_rate\n",
    "    if( self.curr_step < self.burnin):\n",
    "        self.exploration_rate = 1\n",
    "    elif self.curr_step == self.burnin:\n",
    "        self.exploration_rate = self.init_exploration_rate\n",
    "    else:\n",
    "        self.exploration_rate *= self.exploration_rate_decay\n",
    "    self.exploration_rate = max(self.exploration_rate_min, self.exploration_rate)\n",
    "\n",
    "    # increment step\n",
    "    self.curr_step += 1\n",
    "    return action_idx\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NMQT_j9xPDeX"
   },
   "source": [
    "# Cache and Recall\n",
    "\n",
    "These two functions serve as Mario's \"memory\" process. \n",
    "\n",
    "`cache()`: Each time Mario performs an action, he stores the `experience` to his memory. His experience includes the current *state*, *action* performed, *reward* from the action, the *next state*, and whether the game is *done*.\n",
    "\n",
    "`recall()`: Mario randomly samples a batch of experiences from his memory, and uses that to learn the game. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "0BkbT1HZPKJs"
   },
   "outputs": [],
   "source": [
    "class Mario(Mario): # subclassing for continuity\n",
    "  def __init__(self, state_dim, action_dim, save_dir):\n",
    "    super().__init__(state_dim, action_dim, save_dir)\n",
    "    self.memory = deque(maxlen=10000)\n",
    "    self.batch_size = 32\n",
    "    \n",
    "\n",
    "  def cache(self, state, y_pos, next_state, action, reward, done):\n",
    "    \"\"\"\n",
    "    Store the experience to self.memory (replay buffer)\n",
    "\n",
    "    Inputs:\n",
    "    state (LazyFrame),\n",
    "    y_pos (float),\n",
    "    next_state (LazyFrame),\n",
    "    action (int),\n",
    "    reward (float),\n",
    "    done(bool))\n",
    "    \"\"\"\n",
    "    state = np.array(state)\n",
    "    next_state = np.array(next_state)\n",
    "    \n",
    "    state = torch.FloatTensor(state).cuda() if self.use_cuda else torch.FloatTensor(state)\n",
    "    y_pos = torch.FloatTensor([y_pos]).cuda() if self.use_cuda else torch.FloatTensor(state)\n",
    "    next_state = torch.FloatTensor(next_state).cuda() if self.use_cuda else torch.FloatTensor(next_state)\n",
    "    action = torch.LongTensor([action]).cuda() if self.use_cuda else torch.LongTensor([action])\n",
    "    reward = torch.DoubleTensor([reward]).cuda() if self.use_cuda else torch.DoubleTensor([reward])\n",
    "    done = torch.BoolTensor([done]).cuda() if self.use_cuda else torch.BoolTensor([done])\n",
    "\n",
    "    self.memory.append( (state, y_pos, next_state, action, reward, done,) )\n",
    "\n",
    "  \n",
    "  def recall(self):\n",
    "    \"\"\"\n",
    "    Retrieve a batch of experiences from memory\n",
    "    \"\"\"\n",
    "    batch = random.sample(self.memory, self.batch_size)\n",
    "    state, y_pos, next_state, action, reward, done = map(torch.stack, zip(*batch))\n",
    "    return state, y_pos.squeeze(), next_state, action.squeeze(), reward.squeeze(), done.squeeze()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pruDficJzS_A"
   },
   "source": [
    "# Learn\n",
    "\n",
    "Mario uses the [DDQN algorithm](https://arxiv.org/pdf/1509.06461) under the hood. DDQN uses two ConvNets - $Q_{online}$ and $Q_{target}$ - that independently approximate the optimal action-value function. \n",
    "\n",
    "In our implementation, we share feature generator `features` across $Q_{online}$ and $Q_{target}$, but maintain separate FC classifiers for each. $\\theta_{target}$ (the parameters of $Q_{target}$) is frozen to prevent updation by backprop. Instead, it is periodically synced with $\\theta_{online}$ (more on this later)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hKYxjAfTZ2kG"
   },
   "source": [
    "### Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "SeezIHno5cQV"
   },
   "outputs": [],
   "source": [
    "class MarioNet(nn.Module):\n",
    "  '''mini cnn structure\n",
    "  input -> (conv2d + relu) x 3 -> flatten -> (dense + relu) x 2 -> output\n",
    "  '''\n",
    "  def __init__(self, input_dim, output_dim):\n",
    "      super().__init__()\n",
    "      c, h, w = input_dim\n",
    "\n",
    "      if h != 84:\n",
    "          raise ValueError(f\"Expecting input height: 32, got: {h}\")\n",
    "      if w != 84:\n",
    "          raise ValueError(f\"Expecting input width: 32, got: {w}\")\n",
    "\n",
    "#       self.online = nn.Sequential(\n",
    "#           nn.Conv2d(in_channels=c, out_channels=32, kernel_size=5, stride=2, padding=1),\n",
    "#           nn.ReLU(),\n",
    "#           nn.Conv2d(in_channels=32, out_channels=32, kernel_size=3, stride=1),\n",
    "#           nn.ReLU(),\n",
    "#           #nn.MaxPool2d(3),\n",
    "          \n",
    "#           nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, stride=1),\n",
    "#           nn.ReLU(),\n",
    "#           nn.Conv2d(in_channels=64, out_channels=64, kernel_size=3, stride=1),\n",
    "#           nn.MaxPool2d(2),\n",
    "          \n",
    "#           nn.Flatten(),\n",
    "#           nn.Linear(18496, 128),\n",
    "#           nn.ReLU(),\n",
    "#           nn.Linear(128, output_dim)\n",
    "#       )\n",
    "\n",
    "      self.online_features = self.feature_extraction(c, h, w, output_dim)\n",
    "      self.online_td_est  = self.td_est(output_dim)\n",
    "      self.online_aux = self.y_pos_est()\n",
    "\n",
    "      self.target_features = copy.deepcopy(self.online_features)\n",
    "      self.target_td_est = copy.deepcopy(self.online_td_est)\n",
    "        \n",
    "      # Q_target parameters are frozen.\n",
    "      for p in self.target_features.parameters():\n",
    "          p.requires_grad = False\n",
    "      for p in self.target_td_est.parameters():\n",
    "          p.requires_grad = False\n",
    "\n",
    "        \n",
    "  def feature_extraction(self, c, h, w, output_dim):\n",
    "    return nn.Sequential(\n",
    "          nn.Conv2d(in_channels=c, out_channels=32, kernel_size=5, stride=2, padding=1),\n",
    "          nn.ReLU(),\n",
    "          nn.Conv2d(in_channels=32, out_channels=32, kernel_size=3, stride=1),\n",
    "          nn.ReLU(),\n",
    "          #nn.MaxPool2d(3),\n",
    "          \n",
    "          nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, stride=1),\n",
    "          nn.ReLU(),\n",
    "          nn.Conv2d(in_channels=64, out_channels=64, kernel_size=3, stride=1),\n",
    "          nn.MaxPool2d(2),\n",
    "          nn.Flatten()\n",
    "    )\n",
    "\n",
    "  def td_est(self, output_dim):\n",
    "    return nn.Sequential(\n",
    "        nn.Linear(18496, 128),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(128, output_dim)\n",
    "    )\n",
    "\n",
    "  def y_pos_est(self):\n",
    "    return nn.Sequential(\n",
    "        nn.Linear(18496, 32),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(32, 1)\n",
    "    )\n",
    "\n",
    "  def forward(self, input, model):\n",
    "    if model == 'online':\n",
    "        features = self.online_features(input)\n",
    "        return self.online_td_est(features), self.online_aux(features)\n",
    "    elif model == 'target':\n",
    "        features = self.target_features(input)\n",
    "        return self.target_td_est(features), []\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ntDlzPcUkFul"
   },
   "source": [
    "## TD Estimate & TD Target\n",
    "\n",
    "Two values are involved in learning: \n",
    "\n",
    "<br/>\n",
    "\n",
    "**TD Estimate** - the predicted optimal $Q^*$  for a given state $s$ \n",
    "\n",
    "$$\n",
    "{TD}_e = Q_{online}^*(s,a)\n",
    "$$\n",
    "<br/>  \n",
    "\n",
    "\n",
    "**TD Target** - aggregation of current reward and the estimated $Q^*$ in the next state $s'$\n",
    "\n",
    "$$\n",
    "a' = argmax_{a} Q_{online}(s', a)\n",
    "$$\n",
    "$$\n",
    "{TD}_t = r + \\gamma Q_{target}^*(s',a')\n",
    "$$\n",
    "<br/>\n",
    "\n",
    "Because we don’t know what next action $a'$ will be, we use the action $a'$ maximizes $Q_{online}$ in the next state $s'$.\n",
    "\n",
    "Notice we use the [@torch.no_grad()](https://pytorch.org/docs/stable/generated/torch.no_grad.html#no-grad) decorator on `td_target()` to disable gradient calculations here (because we don't need to backpropagate on $\\theta_{target}$).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "YgwTdUHBCSNq"
   },
   "outputs": [],
   "source": [
    "class Mario(Mario):\n",
    "  def __init__(self, state_dim, action_dim, save_dir):\n",
    "    super().__init__(state_dim, action_dim, save_dir)\n",
    "    self.gamma = 0.95\n",
    "    \n",
    "  def td_and_aux_estimate(self, state, action):\n",
    "    outs = self.net(state, model='online')\n",
    "    current_Q = outs[0][np.arange(0, self.batch_size), action] # Q_online(s,a)\n",
    "    y_pos_est = outs[1]\n",
    "    return current_Q, y_pos_est\n",
    "\n",
    "  @torch.no_grad()\n",
    "  def td_target(self, reward, next_state, done):\n",
    "    next_state_Q = self.net(next_state, model='online')[0]\n",
    "    best_action = torch.argmax(next_state_Q, axis=1)\n",
    "    next_Q = self.net(next_state, model='target')[0][np.arange(0, self.batch_size), best_action]\n",
    "    return (reward + (1 - done.float()) * self.gamma * next_Q).float()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BD0m7QwTIM9m"
   },
   "source": [
    "## Updating the model\n",
    "\n",
    "As Mario samples inputs from his replay buffer, we compute $TD_t$ and $TD_e$ and backpropagate this loss down $Q_{online}$ to update its parameters $\\theta_{online}$ ($\\alpha$ is the learning rate `lr` passed to the `Adam optimizer`)\n",
    "$$\n",
    "\\theta_{online} \\leftarrow \\theta_{online} + \\alpha \\nabla(TD_e - TD_t)\n",
    "$$ \n",
    "<br/>\n",
    "$\\theta_{target}$ does not update through backpropagation.\n",
    "Instead, we periodically copy $\\theta_{online}$ to $\\theta_{target}$\n",
    "$$\n",
    "\\theta_{target} \\leftarrow \\theta_{online}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "8F4SxSbFYwLq"
   },
   "outputs": [],
   "source": [
    "class Mario(Mario):\n",
    "    def __init__(self, state_dim, action_dim, save_dir):\n",
    "      super().__init__(state_dim, action_dim, save_dir)\n",
    "      self.optimizer = torch.optim.Adam(self.net.parameters(), lr=0.00025)\n",
    "      self.loss_fn = torch.nn.SmoothL1Loss()\n",
    "\n",
    "    def update_Q_online(self, td_estimate, td_target, y_pos_est, y_pos) :\n",
    "      loss = self.loss_fn(td_estimate, td_target) + (0.2 * self.loss_fn(y_pos_est, y_pos))\n",
    "      self.optimizer.zero_grad()\n",
    "      loss.backward()\n",
    "      self.optimizer.step()\n",
    "      return loss.item()\n",
    "      \n",
    "    def sync_Q_target(self):\n",
    "      self.net.target_features.load_state_dict(self.net.online_features.state_dict())\n",
    "      self.net.target_td_est.load_state_dict(self.net.online_td_est.state_dict())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Sg6vnJFJqjIP"
   },
   "source": [
    "## Save checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "XybP86dNqigo"
   },
   "outputs": [],
   "source": [
    "class Mario(Mario):\n",
    "    def save(self):\n",
    "        save_path = self.save_dir / f\"mario_net_{int(self.curr_step // self.save_every)}.chkpt\"\n",
    "        torch.save(\n",
    "            dict(\n",
    "                model=self.net.state_dict(),\n",
    "                exploration_rate=self.exploration_rate\n",
    "            ),\n",
    "            save_path\n",
    "        )\n",
    "        print(f\"MarioNet saved to {save_path} at step {self.curr_step}\")\n",
    "        \n",
    "    def load(self, load_path):\n",
    "        if not load_path.exists():\n",
    "            raise ValueError(f\"{load_path} does not exist\")\n",
    "\n",
    "        ckp = torch.load(load_path, map_location=('cuda' if self.use_cuda else 'cpu'))\n",
    "        exploration_rate = ckp.get('exploration_rate')\n",
    "        state_dict = ckp.get('model')\n",
    "\n",
    "        print(f\"Loading model at {load_path} with exploration rate {exploration_rate}\")\n",
    "        self.net.load_state_dict(state_dict)\n",
    "        self.exploration_rate = exploration_rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lXjgkNrYlVMD"
   },
   "source": [
    "## Putting it all together\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "OOXBZ9diIMqk"
   },
   "outputs": [],
   "source": [
    "class Mario(Mario):\n",
    "    def __init__(self, state_dim, action_dim, save_dir):\n",
    "        super().__init__(state_dim, action_dim, save_dir)\n",
    "        self.burnin = 5e4  # min. experiences before training\n",
    "        self.learn_every = 3   # no. of experiences between updates to Q_online\n",
    "        self.sync_every = 1e4   # no. of experiences between Q_target & Q_online sync\n",
    "        \n",
    "\n",
    "    def learn(self):\n",
    "      if self.curr_step % self.sync_every == 0:\n",
    "          self.sync_Q_target()\n",
    "\n",
    "      if self.curr_step % self.save_every == 0:\n",
    "          self.save()\n",
    "\n",
    "      if self.curr_step < self.burnin:\n",
    "          return None, None\n",
    "\n",
    "      if self.curr_step % self.learn_every != 0:\n",
    "          return None, None\n",
    "\n",
    "      # Sample from memory\n",
    "      state, y_pos, next_state, action, reward, done = self.recall()\n",
    "\n",
    "      # Get TD Estimate\n",
    "      td_est, y_pos_est = self.td_and_aux_estimate(state, action)\n",
    "\n",
    "      # Get TD Target\n",
    "      td_tgt = self.td_target(reward, next_state, done)\n",
    "\n",
    "      # Backpropagate loss through Q_online\n",
    "      loss = self.update_Q_online(td_est, td_tgt, y_pos_est, torch.reshape(y_pos, y_pos_est.shape))\n",
    "\n",
    "      return (td_est.mean().item(), loss)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nXn31zhJ6uM6"
   },
   "source": [
    "# Logging "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "dHX6MBOB6tNX"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import time, datetime\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "class MetricLogger():\n",
    "    def __init__(self, save_dir):\n",
    "        self.save_log = save_dir / \"log\"\n",
    "        with open(self.save_log, \"w\") as f:\n",
    "            f.write(\n",
    "                f\"{'Episode':>8}{'Step':>8}{'Epsilon':>10}{'MeanReward':>15}\"\n",
    "                f\"{'MeanLength':>15}{'MeanLoss':>15}{'MeanQValue':>15}\"\n",
    "                f\"{'TimeDelta':>15}{'Time':>20}\\n\"\n",
    "            )\n",
    "        self.ep_rewards_plot = save_dir / \"reward_plot.jpg\"\n",
    "        self.ep_lengths_plot = save_dir / \"length_plot.jpg\"\n",
    "        self.ep_avg_losses_plot = save_dir / \"loss_plot.jpg\"\n",
    "        self.ep_avg_qs_plot = save_dir / \"q_plot.jpg\"\n",
    "\n",
    "        # History metrics\n",
    "        self.ep_rewards = []\n",
    "        self.ep_lengths = []\n",
    "        self.ep_avg_losses = []\n",
    "        self.ep_avg_qs = []\n",
    "\n",
    "        # Moving averages, added for every call to record()\n",
    "        self.moving_avg_ep_rewards = []\n",
    "        self.moving_avg_ep_lengths = []\n",
    "        self.moving_avg_ep_avg_losses = []\n",
    "        self.moving_avg_ep_avg_qs = []\n",
    "\n",
    "        # Current episode metric\n",
    "        self.init_episode()\n",
    "\n",
    "        # Timing\n",
    "        self.record_time = time.time()\n",
    "\n",
    "\n",
    "    def log_step(self, reward, loss, q):\n",
    "        self.curr_ep_reward += reward\n",
    "        self.curr_ep_length += 1\n",
    "        if loss:\n",
    "            self.curr_ep_loss += loss\n",
    "            self.curr_ep_q += q\n",
    "            self.curr_ep_loss_length += 1\n",
    "\n",
    "    def log_episode(self):\n",
    "        \"Mark end of episode\"\n",
    "        self.ep_rewards.append(self.curr_ep_reward)\n",
    "        self.ep_lengths.append(self.curr_ep_length)\n",
    "        if self.curr_ep_loss_length == 0:\n",
    "            ep_avg_loss = 0\n",
    "            ep_avg_q = 0\n",
    "        else:\n",
    "            ep_avg_loss = np.round(self.curr_ep_loss / self.curr_ep_loss_length, 5)\n",
    "            ep_avg_q = np.round(self.curr_ep_q / self.curr_ep_loss_length, 5)\n",
    "        self.ep_avg_losses.append(ep_avg_loss)\n",
    "        self.ep_avg_qs.append(ep_avg_q)\n",
    "\n",
    "        self.init_episode()\n",
    "\n",
    "    def init_episode(self):\n",
    "        self.curr_ep_reward = 0.0\n",
    "        self.curr_ep_length = 0\n",
    "        self.curr_ep_loss = 0.0\n",
    "        self.curr_ep_q = 0.0\n",
    "        self.curr_ep_loss_length = 0\n",
    "\n",
    "    def record(self, episode, epsilon, step):\n",
    "        mean_ep_reward = np.round(np.mean(self.ep_rewards[-100:]), 3)\n",
    "        mean_ep_length = np.round(np.mean(self.ep_lengths[-100:]), 3)\n",
    "        mean_ep_loss = np.round(np.mean(self.ep_avg_losses[-100:]), 3)\n",
    "        mean_ep_q = np.round(np.mean(self.ep_avg_qs[-100:]), 3)\n",
    "        self.moving_avg_ep_rewards.append(mean_ep_reward)\n",
    "        self.moving_avg_ep_lengths.append(mean_ep_length)\n",
    "        self.moving_avg_ep_avg_losses.append(mean_ep_loss)\n",
    "        self.moving_avg_ep_avg_qs.append(mean_ep_q)\n",
    "\n",
    "\n",
    "        last_record_time = self.record_time\n",
    "        self.record_time = time.time()\n",
    "        time_since_last_record = np.round(self.record_time - last_record_time, 3)\n",
    "\n",
    "        print(\n",
    "            f\"Episode {episode} - \"\n",
    "            f\"Step {step} - \"\n",
    "            f\"Epsilon {epsilon} - \"\n",
    "            f\"Mean Reward {mean_ep_reward} - \"\n",
    "            f\"Mean Length {mean_ep_length} - \"\n",
    "            f\"Mean Loss {mean_ep_loss} - \"\n",
    "            f\"Mean Q Value {mean_ep_q} - \"\n",
    "            f\"Time Delta {time_since_last_record} - \"\n",
    "            f\"Time {datetime.datetime.now().strftime('%Y-%m-%dT%H:%M:%S')}\"\n",
    "        )\n",
    "\n",
    "        with open(self.save_log, \"a\") as f:\n",
    "            f.write(\n",
    "                f\"{episode:8d}{step:8d}{epsilon:10.3f}\"\n",
    "                f\"{mean_ep_reward:15.3f}{mean_ep_length:15.3f}{mean_ep_loss:15.3f}{mean_ep_q:15.3f}\"\n",
    "                f\"{time_since_last_record:15.3f}\"\n",
    "                f\"{datetime.datetime.now().strftime('%Y-%m-%dT%H:%M:%S'):>20}\\n\"\n",
    "            )\n",
    "\n",
    "        for metric in [\"ep_rewards\", \"ep_lengths\", \"ep_avg_losses\", \"ep_avg_qs\"]:\n",
    "            plt.plot(getattr(self, f\"moving_avg_{metric}\"))\n",
    "            plt.savefig(getattr(self, f\"{metric}_plot\"))\n",
    "            plt.clf()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4QSPezupjA6y"
   },
   "source": [
    "# Let's play!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 477
    },
    "executionInfo": {
     "elapsed": 12940,
     "status": "error",
     "timestamp": 1603853091513,
     "user": {
      "displayName": "Yuansong Feng",
      "photoUrl": "",
      "userId": "12925789853286192206"
     },
     "user_tz": 420
    },
    "id": "UZxl49F6jCzu",
    "outputId": "e8950e87-323e-4694-9117-61a37e47064d",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using CUDA: True\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_35668/3124848373.py:28: DeprecationWarning: In future, it will be an error for 'np.bool_' scalars to be interpreted as an index\n",
      "  done = torch.BoolTensor([done]).cuda() if self.use_cuda else torch.BoolTensor([done])\n",
      "/home/daniel/.local/share/virtualenvs/CM50270_CW2-3mDn6YTX/lib/python3.8/site-packages/gym_super_mario_bros/smb_env.py:148: RuntimeWarning: overflow encountered in ubyte_scalars\n",
      "  return (self.ram[0x86] - self.ram[0x071c]) % 256\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 0 - Step 1635 - Epsilon 1 - Mean Reward 2023.0 - Mean Length 1635.0 - Mean Loss 0.0 - Mean Q Value 0.0 - Time Delta 10.137 - Time 2022-04-27T16:23:17\n",
      "Episode 20 - Step 18077 - Epsilon 1 - Mean Reward 1860.095 - Mean Length 860.81 - Mean Loss 0.0 - Mean Q Value 0.0 - Time Delta 104.008 - Time 2022-04-27T16:25:01\n",
      "Episode 40 - Step 36871 - Epsilon 1 - Mean Reward 1885.78 - Mean Length 899.293 - Mean Loss 0.0 - Mean Q Value 0.0 - Time Delta 120.513 - Time 2022-04-27T16:27:02\n",
      "Episode 60 - Step 53725 - Epsilon 0.6973980465433149 - Mean Reward 1804.393 - Mean Length 880.738 - Mean Loss 0.475 - Mean Q Value 0.153 - Time Delta 125.829 - Time 2022-04-27T16:29:08\n",
      "Episode 80 - Step 68874 - Epsilon 0.6869127794719724 - Mean Reward 1815.235 - Mean Length 850.296 - Mean Loss 1.186 - Mean Q Value 1.087 - Time Delta 175.646 - Time 2022-04-27T16:32:03\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [13]\u001b[0m, in \u001b[0;36m<cell line: 26>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     34\u001b[0m action \u001b[38;5;241m=\u001b[39m mario\u001b[38;5;241m.\u001b[39mact(state)\n\u001b[1;32m     36\u001b[0m \u001b[38;5;66;03m# Agent performs action\u001b[39;00m\n\u001b[0;32m---> 37\u001b[0m next_state, reward, done, info \u001b[38;5;241m=\u001b[39m \u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     39\u001b[0m \u001b[38;5;66;03m# Remember\u001b[39;00m\n\u001b[1;32m     40\u001b[0m mario\u001b[38;5;241m.\u001b[39mcache(state, info[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124my_pos\u001b[39m\u001b[38;5;124m'\u001b[39m], next_state, action, reward, done)\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/CM50270_CW2-3mDn6YTX/lib/python3.8/site-packages/gym/wrappers/frame_stack.py:117\u001b[0m, in \u001b[0;36mFrameStack.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    116\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mstep\u001b[39m(\u001b[38;5;28mself\u001b[39m, action):\n\u001b[0;32m--> 117\u001b[0m     observation, reward, done, info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    118\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mframes\u001b[38;5;241m.\u001b[39mappend(observation)\n\u001b[1;32m    119\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobservation(), reward, done, info\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/CM50270_CW2-3mDn6YTX/lib/python3.8/site-packages/gym/core.py:314\u001b[0m, in \u001b[0;36mObservationWrapper.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    313\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mstep\u001b[39m(\u001b[38;5;28mself\u001b[39m, action):\n\u001b[0;32m--> 314\u001b[0m     observation, reward, done, info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    315\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobservation(observation), reward, done, info\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/CM50270_CW2-3mDn6YTX/lib/python3.8/site-packages/gym/core.py:314\u001b[0m, in \u001b[0;36mObservationWrapper.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    313\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mstep\u001b[39m(\u001b[38;5;28mself\u001b[39m, action):\n\u001b[0;32m--> 314\u001b[0m     observation, reward, done, info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    315\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobservation(observation), reward, done, info\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/CM50270_CW2-3mDn6YTX/lib/python3.8/site-packages/gym/core.py:314\u001b[0m, in \u001b[0;36mObservationWrapper.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    313\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mstep\u001b[39m(\u001b[38;5;28mself\u001b[39m, action):\n\u001b[0;32m--> 314\u001b[0m     observation, reward, done, info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    315\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobservation(observation), reward, done, info\n",
      "Input \u001b[0;32mIn [3]\u001b[0m, in \u001b[0;36mSkipFrame.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     26\u001b[0m done \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_skip):\n\u001b[1;32m     28\u001b[0m     \u001b[38;5;66;03m# Accumulate reward and repeat the same action\u001b[39;00m\n\u001b[0;32m---> 29\u001b[0m     obs, reward, done, info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     30\u001b[0m     total_reward \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m reward\n\u001b[1;32m     31\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m done:\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/CM50270_CW2-3mDn6YTX/lib/python3.8/site-packages/nes_py/wrappers/joypad_space.py:74\u001b[0m, in \u001b[0;36mJoypadSpace.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     60\u001b[0m \u001b[38;5;124;03mTake a step using the given action.\u001b[39;00m\n\u001b[1;32m     61\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     71\u001b[0m \n\u001b[1;32m     72\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     73\u001b[0m \u001b[38;5;66;03m# take the step and record the output\u001b[39;00m\n\u001b[0;32m---> 74\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_action_map\u001b[49m\u001b[43m[\u001b[49m\u001b[43maction\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/CM50270_CW2-3mDn6YTX/lib/python3.8/site-packages/gym/wrappers/time_limit.py:17\u001b[0m, in \u001b[0;36mTimeLimit.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mstep\u001b[39m(\u001b[38;5;28mself\u001b[39m, action):\n\u001b[0;32m---> 17\u001b[0m     observation, reward, done, info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     18\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_elapsed_steps \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m     19\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_elapsed_steps \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_max_episode_steps:\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/CM50270_CW2-3mDn6YTX/lib/python3.8/site-packages/gym/wrappers/order_enforcing.py:13\u001b[0m, in \u001b[0;36mOrderEnforcing.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mstep\u001b[39m(\u001b[38;5;28mself\u001b[39m, action):\n\u001b[1;32m     12\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_has_reset, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot call env.step() before calling reset()\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m---> 13\u001b[0m     observation, reward, done, info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     14\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m observation, reward, done, info\n",
      "File \u001b[0;32m~/.local/share/virtualenvs/CM50270_CW2-3mDn6YTX/lib/python3.8/site-packages/nes_py/nes_env.py:293\u001b[0m, in \u001b[0;36mNESEnv.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    291\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrollers[\u001b[38;5;241m0\u001b[39m][:] \u001b[38;5;241m=\u001b[39m action\n\u001b[1;32m    292\u001b[0m \u001b[38;5;66;03m# pass the action to the emulator as an unsigned byte\u001b[39;00m\n\u001b[0;32m--> 293\u001b[0m \u001b[43m_LIB\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mStep\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_env\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    294\u001b[0m \u001b[38;5;66;03m# get the reward for this step\u001b[39;00m\n\u001b[1;32m    295\u001b[0m reward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_reward()\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "use_cuda = torch.cuda.is_available()\n",
    "\n",
    "if use_cuda:\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "import gc\n",
    "gc.collect()\n",
    "\n",
    "    \n",
    "print(f\"Using CUDA: {use_cuda}\")\n",
    "print()\n",
    "\n",
    "save_dir = Path('checkpoints') / datetime.datetime.now().strftime('%Y-%m-%dT%H-%M-%S')\n",
    "save_dir.mkdir(parents=True)\n",
    "\n",
    "mario = Mario(state_dim=(4, 84, 84), action_dim=env.action_space.n, save_dir=save_dir)\n",
    " \n",
    "# path = Path('checkpoints') / 'checkpoints/2022-04-13T20-59-01/mario_net_11.chkpt'\n",
    "# mario.load(path)\n",
    "\n",
    "logger = MetricLogger(save_dir)\n",
    "\n",
    "episodes = 40000\n",
    "\n",
    "### for Loop that train the model num_episodes times by playing the game\n",
    "for e in range(episodes):\n",
    "\n",
    "    state = env.reset()\n",
    "\n",
    "    # Play the game!\n",
    "    while True:\n",
    "\n",
    "        # Run agent on the state\n",
    "        action = mario.act(state)\n",
    "\n",
    "        # Agent performs action\n",
    "        next_state, reward, done, info = env.step(action)\n",
    "\n",
    "        # Remember\n",
    "        mario.cache(state, info['y_pos'], next_state, action, reward, done)\n",
    "\n",
    "        # Learn\n",
    "        q, loss = mario.learn()\n",
    "\n",
    "        # Logging\n",
    "        logger.log_step(reward, loss, q)\n",
    "\n",
    "        # Update state\n",
    "        state = np.array(next_state)\n",
    "\n",
    "        # Check if end of game\n",
    "        if done or info['flag_get']:\n",
    "            break\n",
    "\n",
    "    logger.log_episode()\n",
    "\n",
    "    if e % 20 == 0:\n",
    "        logger.record(\n",
    "            episode=e,\n",
    "            epsilon=mario.exploration_rate,\n",
    "            step=mario.curr_step\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def render_episode(mario):\n",
    "    \n",
    "    done = True\n",
    "    for step in range(5000):\n",
    "        if done:\n",
    "            state = env.reset()\n",
    "        \n",
    "        # Run agent on the state\n",
    "        action = mario.act(state)\n",
    "\n",
    "        # Agent performs action\n",
    "        next_state, reward, done, info = env.step(action)\n",
    "        \n",
    "        env.render()\n",
    "        \n",
    "# mario = Mario(state_dim=(4, 84, 84), action_dim=env.action_space.n, save_dir=save_dir)\n",
    "# path = Path('checkpoints') / 'mario_net_12.chkpt'\n",
    "# mario.load(path)\n",
    "mario.exploration_rate = 0\n",
    "render_episode(mario)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "hKYxjAfTZ2kG",
    "ntDlzPcUkFul"
   ],
   "name": "tutorial_v2.ipynb",
   "provenance": [
    {
     "file_id": "1CGyf0EV3Pm7VyPseb--M7d6IvsaeMqIE",
     "timestamp": 1603853250256
    },
    {
     "file_id": "1IdPk7OJU9QycV3fH6lt8IFMReXvKLi2B",
     "timestamp": 1603853124090
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
