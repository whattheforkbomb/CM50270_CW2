{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "v0mrHhpYZzxP"
   },
   "source": [
    "# AI-powered Mario\n",
    "\n",
    "Authors: [Yuansong Feng](https://github.com/YuansongFeng), [Suraj Subramanian](https://github.com/suraj813), [Howard Wang](https://github.com/hw26), [Steven Guo](https://github.com/GuoYuzhang).  \n",
    "\n",
    "\n",
    "## Welcome!\n",
    "This tutorial walks you through the fundamentals of Deep Reinforcement Learning. At the end, you will implement an AI-powered Mario (using [Double Deep Q-Networks](https://arxiv.org/pdf/1509.06461.pdf)) that can play the game by itself. \n",
    "\n",
    "Although no prior knowledge of RL is necessary for this tutorial, you can familiarize yourself with these RL [concepts](https://spinningup.openai.com/en/latest/spinningup/rl_intro.html), and have this handy [cheatsheet](https://colab.research.google.com/drive/1eN33dPVtdPViiS1njTW_-r-IYCDTFU7N) as your companion. The full code is available [here](https://github.com/yuansongFeng/MadMario/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SNstQbV0__cX"
   },
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 459
    },
    "executionInfo": {
     "elapsed": 15774,
     "status": "ok",
     "timestamp": 1603853036176,
     "user": {
      "displayName": "Yuansong Feng",
      "photoUrl": "",
      "userId": "12925789853286192206"
     },
     "user_tz": 420
    },
    "id": "_1NdXf43AUfY",
    "outputId": "ae4a2b61-0f4f-4b47-9a6e-27a2dcbe9cdd"
   },
   "outputs": [],
   "source": [
    "# Mario game environment\n",
    "#!pip install gym-super-mario-bros==7.3.0 opencv-python\n",
    "\n",
    "import os\n",
    "import copy\n",
    "import torch\n",
    "from pathlib import Path\n",
    "from collections import deque\n",
    "\n",
    "from Agent import Mario\n",
    "from Logger import MetricLogger\n",
    "import random, datetime, numpy as np, cv2 \n",
    "# Gym is an OpenAI toolkit for RL\n",
    "import gym\n",
    "from gym.spaces import Box\n",
    "from gym.wrappers import FrameStack, GrayScaleObservation, TransformObservation\n",
    "\n",
    "#NES Emulator for OpenAI Gym\n",
    "from nes_py.wrappers import JoypadSpace\n",
    "\n",
    "# Super Mario environment for OpenAI Gym\n",
    "import gym_super_mario_bros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(240, 256, 3),\n",
      " 0,\n",
      " False,\n",
      " {'coins': 0, 'flag_get': False, 'life': 2, 'score': 0, 'stage': 1, 'status': 'small', 'time': 400, 'world': 1, 'x_pos': 40, 'x_pos_screen': 40, 'y_pos': 79}\n"
     ]
    }
   ],
   "source": [
    "# Initialize Super Mario environment\n",
    "env = gym_super_mario_bros.make('SuperMarioBros-v3')\n",
    "\n",
    "# Limit the action-space to\n",
    "#   0. walk right \n",
    "#   1. jump right\n",
    "env = JoypadSpace(\n",
    "    env,\n",
    "    [['right'],\n",
    "    ['right', 'A']]\n",
    ")\n",
    "\n",
    "env.reset()\n",
    "next_state, reward, done, info = env.step(action=0)\n",
    "print(f'{next_state.shape},\\n {reward},\\n {done},\\n {info}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "GaKjHlNfGNMC"
   },
   "outputs": [],
   "source": [
    "class ResizeObservation(gym.ObservationWrapper):\n",
    "    def __init__(self, env, shape):\n",
    "        super().__init__(env)\n",
    "        if isinstance(shape, int):\n",
    "            self.shape = (shape, shape)\n",
    "        else:\n",
    "            self.shape = tuple(shape)\n",
    "\n",
    "        obs_shape = self.shape + self.observation_space.shape[2:]\n",
    "        self.observation_space = Box(low=0, high=255, shape=obs_shape, dtype=np.uint8)\n",
    "\n",
    "    def observation(self, observation):\n",
    "        observation = cv2.resize(observation, self.shape, interpolation=cv2.INTER_AREA)\n",
    "        return observation\n",
    "\n",
    "\n",
    "class SkipFrame(gym.Wrapper):\n",
    "    def __init__(self, env, skip):\n",
    "        \"\"\"Return only every `skip`-th frame\"\"\"\n",
    "        super().__init__(env)\n",
    "        self._skip = skip\n",
    "\n",
    "    def step(self, action):\n",
    "        \"\"\"Repeat action, and sum reward\"\"\"\n",
    "        total_reward = 0.0\n",
    "        done = False\n",
    "        for i in range(self._skip):\n",
    "            # Accumulate reward and repeat the same action\n",
    "            obs, reward, done, info = self.env.step(action)\n",
    "            total_reward += reward\n",
    "            if done:\n",
    "                break\n",
    "        return obs, total_reward, done, info\n",
    "\n",
    "\n",
    "# Apply Wrappers to environment\n",
    "env = SkipFrame(env, skip=4)\n",
    "env = GrayScaleObservation(env, keep_dim=False)\n",
    "env = ResizeObservation(env, shape=84)\n",
    "env = TransformObservation(env, f=lambda x: x / 255.)\n",
    "env = FrameStack(env, num_stack=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nXn31zhJ6uM6"
   },
   "source": [
    "# Logging "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dHX6MBOB6tNX"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4QSPezupjA6y"
   },
   "source": [
    "# Let's play!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 477
    },
    "executionInfo": {
     "elapsed": 12940,
     "status": "error",
     "timestamp": 1603853091513,
     "user": {
      "displayName": "Yuansong Feng",
      "photoUrl": "",
      "userId": "12925789853286192206"
     },
     "user_tz": 420
    },
    "id": "UZxl49F6jCzu",
    "outputId": "e8950e87-323e-4694-9117-61a37e47064d",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using CUDA: True\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/daniel/CM50270_CW2/src/Agent.py:109: DeprecationWarning: In future, it will be an error for 'np.bool_' scalars to be interpreted as an index\n",
      "  done = torch.BoolTensor([done]).cuda() if self.use_cuda else torch.BoolTensor([done])\n",
      "/home/daniel/.local/share/virtualenvs/CM50270_CW2-3mDn6YTX/lib/python3.8/site-packages/gym_super_mario_bros/smb_env.py:148: RuntimeWarning: overflow encountered in ubyte_scalars\n",
      "  return (self.ram[0x86] - self.ram[0x071c]) % 256\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 0 - Step 1014 - Epsilon 1 - Mean Reward 2373.0 - Mean Length 1014.0 - Mean Loss 0.0 - Mean Q Value 0.0 - Time Delta 6.437 - Time 2022-04-27T17:07:11\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA out of memory. Tried to allocate 2.00 MiB (GPU 0; 5.81 GiB total capacity; 1.37 GiB already allocated; 3.31 MiB free; 1.42 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Input \u001b[0;32mIn [4]\u001b[0m, in \u001b[0;36m<cell line: 26>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     37\u001b[0m next_state, reward, done, info \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mstep(action)\n\u001b[1;32m     39\u001b[0m \u001b[38;5;66;03m# Remember\u001b[39;00m\n\u001b[0;32m---> 40\u001b[0m \u001b[43mmario\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcache\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minfo\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43my_pos\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnext_state\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maction\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreward\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdone\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     42\u001b[0m \u001b[38;5;66;03m# Learn\u001b[39;00m\n\u001b[1;32m     43\u001b[0m q, loss \u001b[38;5;241m=\u001b[39m mario\u001b[38;5;241m.\u001b[39mlearn()\n",
      "File \u001b[0;32m~/CM50270_CW2/src/Agent.py:106\u001b[0m, in \u001b[0;36mMario.cache\u001b[0;34m(self, state, y_pos, next_state, action, reward, done)\u001b[0m\n\u001b[1;32m    104\u001b[0m state \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mFloatTensor(state)\u001b[38;5;241m.\u001b[39mcuda() \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39muse_cuda \u001b[38;5;28;01melse\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mFloatTensor(state)\n\u001b[1;32m    105\u001b[0m y_pos \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mFloatTensor([y_pos])\u001b[38;5;241m.\u001b[39mcuda() \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39muse_cuda \u001b[38;5;28;01melse\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mFloatTensor(state)\n\u001b[0;32m--> 106\u001b[0m next_state \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mFloatTensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnext_state\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcuda\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39muse_cuda \u001b[38;5;28;01melse\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mFloatTensor(next_state)\n\u001b[1;32m    107\u001b[0m action \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mLongTensor([action])\u001b[38;5;241m.\u001b[39mcuda() \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39muse_cuda \u001b[38;5;28;01melse\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mLongTensor([action])\n\u001b[1;32m    108\u001b[0m reward \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mDoubleTensor([reward])\u001b[38;5;241m.\u001b[39mcuda() \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39muse_cuda \u001b[38;5;28;01melse\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mDoubleTensor([reward])\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 2.00 MiB (GPU 0; 5.81 GiB total capacity; 1.37 GiB already allocated; 3.31 MiB free; 1.42 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "use_cuda = torch.cuda.is_available()\n",
    "\n",
    "if use_cuda:\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "import gc\n",
    "gc.collect()\n",
    "\n",
    "    \n",
    "print(f\"Using CUDA: {use_cuda}\")\n",
    "print()\n",
    "\n",
    "save_dir = Path('checkpoints') / datetime.datetime.now().strftime('%Y-%m-%dT%H-%M-%S')\n",
    "save_dir.mkdir(parents=True)\n",
    "\n",
    "mario = Mario((4, 84, 84), env.action_space.n, save_dir)\n",
    " \n",
    "# path = Path('checkpoints') / 'checkpoints/2022-04-13T20-59-01/mario_net_11.chkpt'\n",
    "# mario.load(path)\n",
    "\n",
    "logger = MetricLogger(save_dir)\n",
    "\n",
    "episodes = 40000\n",
    "\n",
    "### for Loop that train the model num_episodes times by playing the game\n",
    "for e in range(episodes):\n",
    "\n",
    "    state = env.reset()\n",
    "\n",
    "    # Play the game!\n",
    "    while True:\n",
    "\n",
    "        # Run agent on the state\n",
    "        action = mario.act(state)\n",
    "\n",
    "        # Agent performs action\n",
    "        next_state, reward, done, info = env.step(action)\n",
    "\n",
    "        # Remember\n",
    "        mario.cache(state, info['y_pos'], next_state, action, reward, done)\n",
    "\n",
    "        # Learn\n",
    "        q, loss = mario.learn()\n",
    "\n",
    "        # Logging\n",
    "        logger.log_step(reward, loss, q)\n",
    "\n",
    "        # Update state\n",
    "        state = np.array(next_state)\n",
    "\n",
    "        # Check if end of game\n",
    "        if done or info['flag_get']:\n",
    "            break\n",
    "\n",
    "    logger.log_episode()\n",
    "\n",
    "    if e % 20 == 0:\n",
    "        logger.record(\n",
    "            episode=e,\n",
    "            epsilon=mario.exploration_rate,\n",
    "            step=mario.curr_step\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def render_episode(mario):\n",
    "    \n",
    "    done = True\n",
    "    for step in range(5000):\n",
    "        if done:\n",
    "            state = env.reset()\n",
    "        \n",
    "        # Run agent on the state\n",
    "        action = mario.act(state)\n",
    "\n",
    "        # Agent performs action\n",
    "        next_state, reward, done, info = env.step(action)\n",
    "        \n",
    "        env.render()\n",
    "        \n",
    "# mario = Mario(state_dim=(4, 84, 84), action_dim=env.action_space.n, save_dir=save_dir)\n",
    "# path = Path('checkpoints') / 'mario_net_12.chkpt'\n",
    "# mario.load(path)\n",
    "mario.exploration_rate = 0\n",
    "render_episode(mario)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "hKYxjAfTZ2kG",
    "ntDlzPcUkFul"
   ],
   "name": "tutorial_v2.ipynb",
   "provenance": [
    {
     "file_id": "1CGyf0EV3Pm7VyPseb--M7d6IvsaeMqIE",
     "timestamp": 1603853250256
    },
    {
     "file_id": "1IdPk7OJU9QycV3fH6lt8IFMReXvKLi2B",
     "timestamp": 1603853124090
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
