{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e3e1f006-802d-4325-a74a-3c3f78e31c23",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2fd2055f-ea63-42d7-8958-2c5b5fe72a6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/whiffingj/bin/anaconda3/envs/rl_vis/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "updating path to include: /home/whiffingj/dev/uni/CM50270_CW2/src/DQN\n",
      "updating path to include: /home/whiffingj/dev/uni/CM50270_CW2/src/a2c\n"
     ]
    }
   ],
   "source": [
    "# Super Mario Bros env dependencies\n",
    "from nes_py.wrappers import JoypadSpace\n",
    "import gym_super_mario_bros\n",
    "from gym_super_mario_bros.actions import SIMPLE_MOVEMENT, RIGHT_ONLY\n",
    "import gym\n",
    "from gym.spaces import Box\n",
    "from gym.wrappers import FrameStack, GrayScaleObservation, TransformObservation\n",
    "\n",
    "# Torch\n",
    "import torch\n",
    "import torch.utils\n",
    "import torch.nn as nn\n",
    "\n",
    "# Networks to Evaluate\n",
    "import sys, os\n",
    "def add_to_path(model_dir):\n",
    "    notebook_file = os.path.dirname(\"CNN_Feature_Visualisation.ipynb\")\n",
    "    path2add = os.path.normpath(os.path.abspath(os.path.join(notebook_file, os.path.pardir, model_dir)))\n",
    "    if (not (path2add in sys.path)):\n",
    "        print(f'updating path to include: {path2add}')\n",
    "        sys.path.append(path2add)\n",
    "add_to_path('DQN')\n",
    "from OldAgent import MarioNet, Mario\n",
    "add_to_path('a2c')\n",
    "from a2c.model import ACNetwork\n",
    "\n",
    "# CNN Visualisation (Lucent)\n",
    "from lucent.optvis import render, param, transform, objectives\n",
    "from lucent.misc.io import show\n",
    "from lucent.modelzoo.util import get_model_layers\n",
    "\n",
    "# Utilities\n",
    "from utils.wrappers import ResizeObservation, SkipFrame\n",
    "import utils.helper\n",
    "from utils.config import Config\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "from pathlib import Path\n",
    "import torch.utils\n",
    "import torch._utils"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a1d831b",
   "metadata": {},
   "source": [
    "## Model and Env Prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e5c0cc1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Is cuda supported on the system? Yes\n"
     ]
    }
   ],
   "source": [
    "def create_env(random = False, movement = SIMPLE_MOVEMENT):\n",
    "    env_name = \"SuperMarioBros\"\n",
    "    if random:\n",
    "        env_name += \"RandomStage\"\n",
    "    env_name += '-v3'\n",
    "    env = gym_super_mario_bros.make(env_name)\n",
    "    env = JoypadSpace(env, movement)\n",
    "    return env, env_name\n",
    "\n",
    "def run_model(env, steps, get_action, get_obs, image_dir):\n",
    "    state = env.reset()\n",
    "    frame = 0    \n",
    "    for step in range(steps):\n",
    "        obs = get_obs(state)\n",
    "        if len(obs.shape) == 3 and obs.shape[0] == 4:\n",
    "                obs = np.concatenate(obs, axis=0) # if we have 4 inputs, we want them organised left to right\n",
    "        Image.fromarray(obs).save(os.path.join(image_dir, f\"frame_{frame}.png\"))\n",
    "        frame += 1\n",
    "\n",
    "        action = get_action(state)\n",
    "        next_state, reward, done, _ = env.step(action)\n",
    "        state = next_state\n",
    "        \n",
    "        if done:\n",
    "            state = env.reset()\n",
    "\n",
    "print(f\"Is cuda supported on the system? {'Yes' if torch.cuda.is_available() else 'No'}\")\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e790ccc2",
   "metadata": {},
   "source": [
    "## Setup Feature Visualisation Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f786b61e-e884-4882-8d74-ea339b216ecb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Code derived from: https://colab.research.google.com/github/greentfrapp/lucent-notebooks/blob/master/notebooks/feature_inversion.ipynb#scrollTo=d47pkOPKvNjs\n",
    "@objectives.wrap_objective()\n",
    "def dot_compare(layer, batch=1, cossim_pow=0):\n",
    "    def inner(T):\n",
    "        dot = (T(layer)[batch] * T(layer)[0]).sum()\n",
    "        mag = torch.sqrt(torch.sum(T(layer)[0]**2))\n",
    "        cossim = dot/(1e-6 + mag)\n",
    "        return -dot * cossim ** cossim_pow\n",
    "    return inner\n",
    "\n",
    "transforms = [\n",
    "    transform.pad(8, mode='constant', constant_value=.5),\n",
    "    transform.jitter(8),\n",
    "    transform.random_scale([0.9, 0.95, 1.05, 1.1] + [1]*4),\n",
    "    transform.random_rotate(list(range(-5, 5)) + [0]*5),\n",
    "    transform.jitter(2),\n",
    "]\n",
    "\n",
    "def get_param_f(img, device):\n",
    "    img = torch.tensor(img).to(device)\n",
    "    # Initialize parameterized input and stack with target image\n",
    "    # to be accessed in the objective function\n",
    "    params, image_f = param.image(img.shape[1], channels=img.shape[0])\n",
    "    def stacked_param_f():\n",
    "        return params, lambda: torch.stack([image_f()[0], img])\n",
    "\n",
    "    return stacked_param_f\n",
    "\n",
    "def feature_inversion(img, device, layer, model, n_steps=512, cossim_pow=0.0):  \n",
    "    obj = objectives.Objective.sum([\n",
    "        1.0 * dot_compare(layer, cossim_pow=cossim_pow),\n",
    "        objectives.blur_input_each_step(),\n",
    "    ])\n",
    "\n",
    "    param_f = get_param_f(img, device)\n",
    "    images = render.render_vis(model, obj, param_f, transforms=transforms, preprocess=False, thresholds=(n_steps,), show_image=False, progress=False)\n",
    "    return images\n",
    "\n",
    "def add_image_to_figure(ax, image, description, fig_args=dict()):\n",
    "    ax.imshow(image, **fig_args) # Input frames\n",
    "    ax.yaxis.set_visible(False)\n",
    "    ax.tick_params(axis='x', which='both', bottom=False, top=False, labelbottom=False)\n",
    "    ax.set_xlabel(description)\n",
    "\n",
    "def visualise_cnn_layer_neurons(src_img_path, model, layers, device, convergence_steps, desc, out_img_path):\n",
    "    if src_img_path != \"NOISE\":\n",
    "        stacked_image = np.array(Image.open(src_img_path), np.float32)\n",
    "        image = stacked_image.reshape((4, 84, 84))\n",
    "    else:\n",
    "        stacked_image = np.random.uniform(0, 255, (4, 84, 84))\n",
    "        image = stacked_image\n",
    "\n",
    "    processed_layers = []\n",
    "    for layer_id, neurons, layer_idx in layers:\n",
    "        print(f\"Processing Layer-{layer_idx}\")\n",
    "        channel_activation = feature_inversion(image, device, layer_id, model, n_steps=convergence_steps)[0]\n",
    "\n",
    "        neuron_activations = []\n",
    "        param_f = get_param_f(image, device)\n",
    "        for neuron in range(neurons):\n",
    "            obj = f\"{layer_id}:{neuron}\"\n",
    "            # print(f\"Generating Activation Image For Neuron-{neuron}\")\n",
    "            neuron_activations.append(render.render_vis(model, obj, param_f, transforms=transforms, preprocess=False, thresholds=(convergence_steps,), show_image=False, progress=False)[0])\n",
    "\n",
    "        processed_layers.append((channel_activation, neuron_activations))\n",
    "\n",
    "    max_val = -np.inf\n",
    "    min_val = np.inf\n",
    "    activation_images = []\n",
    "    for channel_act, neurons_acts in processed_layers:\n",
    "        # get activation image\n",
    "        processed_channel_act = channel_act[0].transpose([2, 0, 1])\n",
    "        current_max = np.max(processed_channel_act)\n",
    "        current_min = np.min(processed_channel_act)\n",
    "\n",
    "        processed_neurons_acts = []\n",
    "        for neurons_act in neurons_acts:\n",
    "            processed_neurons_act = neurons_act[0].transpose([2, 0, 1])\n",
    "            processed_neurons_acts.append(processed_neurons_act)\n",
    "            current_n_max = np.max(processed_neurons_act)\n",
    "            current_n_min = np.min(processed_neurons_act)\n",
    "            if current_n_max > max_val:\n",
    "                max_val = current_n_max\n",
    "            if current_n_min < current_min:\n",
    "                current_min = current_n_min\n",
    "\n",
    "        if current_max > current_max:\n",
    "            current_max = current_max\n",
    "        if current_min < min_val:\n",
    "            min_val = current_min\n",
    "\n",
    "        activation_images.append((processed_channel_act, processed_neurons_acts))\n",
    "\n",
    "    plt.rcParams['figure.dpi'] = 200\n",
    "    fig = plt.figure(constrained_layout=True, figsize=(6, 8))\n",
    "    total_cols = 5\n",
    "    total_rows = len(layers)+1#int(np.sum([l[1] for l in layers]) / total_cols) + len(layers) + 1\n",
    "    gs = fig.add_gridspec(ncols=total_cols, nrows=total_rows)\n",
    "    input_ax = fig.add_subplot(gs[0,:])\n",
    "    add_image_to_figure(ax=input_ax, image=np.concatenate(image, axis=1), description=\"Input Frame Stack (Left -> Right: Frames 0-3)\", fig_args=dict(cmap='gray', vmin=0, vmax=255))\n",
    "    for _, neurons, layer_idx in layers:\n",
    "        print(f\"Processing Activation Image For Layer-{layer_idx}, {neurons}\")\n",
    "        channel_activation, neuron_activations = activation_images[layer_idx]\n",
    "        channel_rgba_ax = fig.add_subplot(gs[layer_idx+1,0])\n",
    "        channel_stacked_ax = fig.add_subplot(gs[layer_idx+1,1:])\n",
    "        add_image_to_figure(ax=channel_rgba_ax, image=channel_activation.transpose([1, 2, 0]), description=f\"ConvLayer-{layer_idx}-RGBA\")\n",
    "        add_image_to_figure(ax=channel_stacked_ax, image=np.concatenate(channel_activation, axis=1), description=f\"ConvLayer-{layer_idx}-Stacked\", fig_args=dict(cmap='plasma', vmin=min_val, vmax=max_val))\n",
    "        \n",
    "        neurons_fig = plt.figure(figsize=(18, 14))\n",
    "        neurons_total_cols = 8\n",
    "        neurons_total_rows = int(neurons / neurons_total_cols)\n",
    "        neurons_gs = fig.add_gridspec(ncols=neurons_total_cols, nrows=neurons_total_rows)\n",
    "        for neuron_x in range(neurons_total_cols):\n",
    "            for neuron_y in range(neurons_total_rows):\n",
    "                neuron_activation = neuron_activations[(neuron_x*neurons_total_rows) + neuron_y]\n",
    "                neuron_rgba_ax = neurons_fig.add_subplot(neurons_gs[neuron_y,neuron_x])\n",
    "                add_image_to_figure(ax=neuron_rgba_ax, image=neuron_activation.transpose([1, 2, 0]), description=f\"Neuron-{neuron_x + (neuron_y*neurons_total_rows)}-RGBA\")\n",
    "        neurons_fig.suptitle(f\"RGBA Neurons For Layer {layer_idx} - {desc}\")\n",
    "        neurons_fig.tight_layout()\n",
    "        # neurons_fig.show()\n",
    "        neurons_fig.savefig(f\"{out_img_path}_layer-{layer_idx}-neurons-rgba.png\")\n",
    "        neurons_fig.clf()\n",
    "\n",
    "        neurons_fig = plt.figure(figsize=(18, 14))\n",
    "        neurons_total_cols = 16\n",
    "        neurons_total_rows = int(neurons / 4)\n",
    "        neurons_gs = fig.add_gridspec(ncols=neurons_total_cols, nrows=neurons_total_rows)\n",
    "        for neuron_x in range(4):\n",
    "            for neuron_y in range(neurons_total_rows):\n",
    "                neuron_activation = neuron_activations[(neuron_x*neurons_total_rows) + neuron_y]\n",
    "                neuron_stacked_ax = neurons_fig.add_subplot(neurons_gs[neuron_y,(neuron_x*4):(neuron_x*4)+4])\n",
    "                add_image_to_figure(ax=neuron_stacked_ax, image=np.concatenate(neuron_activation, axis=1), description=f\"Neuron-{neuron_x + (neuron_y*neurons_total_rows)}-Stacked\", fig_args=dict(cmap='plasma', vmin=min_val, vmax=max_val))\n",
    "        neurons_fig.suptitle(f\"Neurons For Layer {layer_idx} - {desc}\")\n",
    "        neurons_fig.tight_layout()\n",
    "        # neurons_fig.show()\n",
    "        neurons_fig.savefig(f\"{out_img_path}_layer-{layer_idx}-neurons.png\")\n",
    "        neurons_fig.clf()\n",
    "\n",
    "    fig.suptitle(f\"Extracted Layer Features - {desc}\")\n",
    "    fig.tight_layout()\n",
    "    # fig.show()\n",
    "    fig.savefig(f\"{out_img_path}.png\")\n",
    "    fig.clf()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "183a6dfe",
   "metadata": {},
   "source": [
    "## Generate Some Runs\n",
    "To infer what the model has learnt, we need to get the observations that are fed into the CNN\n",
    "\n",
    "This doesn't need to be run if the images already exist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0774a093",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input frames already exist, skipping generation...\n"
     ]
    }
   ],
   "source": [
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "input_img_path = 'input_frame_stacks'\n",
    "onlyfiles = [f for f in listdir(input_img_path) if isfile(join(input_img_path, f))] # get files in path\n",
    "if not onlyfiles:\n",
    "    print(\"Generating input frames\")\n",
    "    # Using A2C as it is able to get further\n",
    "    env, env_name = create_env()\n",
    "\n",
    "    # Apply wrappers to environment\n",
    "    env = SkipFrame(env, skip=4)\n",
    "    env = GrayScaleObservation(env, keep_dim=False) # Grayscale images\n",
    "    env = ResizeObservation(env, shape=84) # image dim: [84, 84]\n",
    "    env = FrameStack(env, num_stack=4) # 4 frames at a time\n",
    "    obs = (4, 84, 84)\n",
    "\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = ACNetwork(obs, env.action_space.n)\n",
    "    checkpoint = torch.load('checkpoints/a2c/a2c_rollout10_ep120k.pt', map_location=device)\n",
    "    model.load_state_dict(checkpoint['model'])\n",
    "    model.cuda()\n",
    "    model.eval()\n",
    "    print(model)\n",
    "    model_cnn = model.conv\n",
    "\n",
    "    # run some steps\n",
    "    def get_action(state):\n",
    "        state = helper.normalize_states(helper.to_tensor(state)).to(device)\n",
    "        action_probs = model.forward(state.unsqueeze(0))[0]\n",
    "        return torch.distributions.Categorical(action_probs).sample().item()\n",
    "\n",
    "    def get_obs(state):\n",
    "        return np.array(state)\n",
    "\n",
    "    run_model(env, 300, get_action, get_obs, input_img_path)\n",
    "    env.close()\n",
    "else:\n",
    "    print(\"Input frames already exist, skipping generation...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b2f9589",
   "metadata": {},
   "source": [
    "## A2C\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a236789e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading A2C model from checkpoint: checkpoints/a2c/a2c_rollout10_ep120k.pt\n",
      "Model Loaded, Ensuring weights setup for cuda\n",
      "Evaluating model ready for CNN Feature Visualisation\n",
      "Sequential(\n",
      "  (0): Conv2d(4, 32, kernel_size=(5, 5), stride=(2, 2), padding=(1, 1))\n",
      "  (1): ReLU()\n",
      "  (2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (3): ReLU()\n",
      "  (4): MaxPool2d(kernel_size=3, stride=3, padding=0, dilation=1, ceil_mode=False)\n",
      "  (5): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (6): ReLU()\n",
      "  (7): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (8): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      ")\n",
      "Processing Layer-0\n",
      "Interrupted optimization at step 941.\n"
     ]
    }
   ],
   "source": [
    "env, env_name = create_env()\n",
    "\n",
    "# Apply wrappers to environment\n",
    "env = SkipFrame(env, skip=4)\n",
    "env = GrayScaleObservation(env, keep_dim=False) # Grayscale images\n",
    "env = ResizeObservation(env, shape=84) # image dim: [84, 84]\n",
    "env = FrameStack(env, num_stack=4) # 4 frames at a time\n",
    "obs = (4, 84, 84)\n",
    "\n",
    "\n",
    "for model_eps in ['120k']:#['100k', '120k']:\n",
    "    model = ACNetwork(obs, env.action_space.n)\n",
    "    model_path = f'checkpoints/a2c/a2c_rollout10_ep{model_eps}.pt'\n",
    "    print(f\"Loading A2C model from checkpoint: {model_path}\")\n",
    "    checkpoint = torch.load(model_path, map_location=device)\n",
    "    model.load_state_dict(checkpoint['model'])\n",
    "    print(\"Model Loaded, Ensuring weights setup for cuda\")\n",
    "    model.cuda()\n",
    "    print(\"Evaluating model ready for CNN Feature Visualisation\")\n",
    "    model.eval()\n",
    "    model_cnn = model.conv\n",
    "    print(model_cnn)\n",
    "\n",
    "    # Frames\n",
    "    input_img_path = 'input_frame_stacks'\n",
    "    frames = [\n",
    "        ('frame_42', \"Jumping Over Goomba\"),\n",
    "        ('frame_70', \"Jumping Over Pipe\"),\n",
    "        ('frame_108',\"Falling In-front Of Goomba (In Air)\"),\n",
    "        ('frame_111',\"Falling In-front Of Goomba\"),\n",
    "        ('frame_230',\"Stuck On Pipe\"),\n",
    "        ('frame_233',\"Stuck On Pipe (In Air)\")\n",
    "    ]\n",
    "\n",
    "    # visualise_cnn_layers\n",
    "    n_step = 2048\n",
    "    conv_layers = [('0', 32, 0), ('2', 32, 1), ('7', 64, 2)] # (layer_idx, features, conv_layer_idx)\n",
    "    for file_id, desc in frames:\n",
    "        if file_id != \"NOISE\":\n",
    "            frame_path = os.path.join(input_img_path, f\"{file_id}.png\")\n",
    "        else:\n",
    "            frame_path = file_id\n",
    "        visualise_cnn_layer_neurons(frame_path, model_cnn, conv_layers, device, n_step, f\"CNN Extracted Features Visualisation\\nA2C Model ({model_eps} Episodes) - Scenario: {desc}\", f\"out/A2C/{model_eps}ep_{n_step}-steps_{desc.replace(' ', '-')}\")\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4318514",
   "metadata": {},
   "source": [
    "## DQN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ad18a40e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading DQN model from checkpoint: checkpoints/dqn/mario_net_8.chkpt\n",
      "Loading model at checkpoints/dqn/mario_net_8.chkpt with exploration rate 0.19691163516176424\n",
      "Model Loaded, Evaluating model ready for CNN Feature Visualisation\n",
      "Sequential(\n",
      "  (0): Conv2d(4, 32, kernel_size=(5, 5), stride=(2, 2), padding=(1, 1))\n",
      "  (1): ReLU()\n",
      "  (2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (3): ReLU()\n",
      "  (4): MaxPool2d(kernel_size=3, stride=3, padding=0, dilation=1, ceil_mode=False)\n",
      "  (5): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (6): ReLU()\n",
      "  (7): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (8): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (9): Flatten(start_dim=1, end_dim=-1)\n",
      "  (10): Linear(in_features=1024, out_features=128, bias=True)\n",
      "  (11): ReLU()\n",
      "  (12): Linear(in_features=128, out_features=3, bias=True)\n",
      ")\n",
      "Processing Layer-0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/whiffingj/bin/anaconda3/envs/rl_vis/lib/python3.9/site-packages/lucent/optvis/render.py:103: UserWarning: Some layers could not be computed because the size of the image is not big enough. It is fine, as long as the noncomputed layers are not used in the objective function(exception details: 'mat1 and mat2 shapes cannot be multiplied (2x16384 and 1024x128)')\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing Layer-1\n",
      "Processing Layer-2\n",
      "Processing Layer-3\n",
      "Processing Activation Image For Layer-0, 32\n",
      "Processing Activation Image For Layer-1, 32\n",
      "Processing Activation Image For Layer-2, 64\n",
      "Processing Activation Image For Layer-3, 64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_20159/1798537024.py:134: UserWarning: This figure was using constrained_layout, but that is incompatible with subplots_adjust and/or tight_layout; disabling constrained_layout.\n",
      "  fig.tight_layout()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing Layer-0\n",
      "Processing Layer-1\n",
      "Processing Layer-2\n",
      "Processing Layer-3\n",
      "Processing Activation Image For Layer-0, 32\n",
      "Processing Activation Image For Layer-1, 32\n",
      "Processing Activation Image For Layer-2, 64\n",
      "Processing Activation Image For Layer-3, 64\n",
      "Processing Layer-0\n",
      "Processing Layer-1\n",
      "Processing Layer-2\n",
      "Processing Layer-3\n",
      "Processing Activation Image For Layer-0, 32\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_20159/1798537024.py:119: RuntimeWarning: More than 20 figures have been opened. Figures created through the pyplot interface (`matplotlib.pyplot.figure`) are retained until explicitly closed and may consume too much memory. (To control this warning, see the rcParam `figure.max_open_warning`).\n",
      "  neurons_fig = plt.figure(figsize=(18, 14))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing Activation Image For Layer-1, 32\n",
      "Processing Activation Image For Layer-2, 64\n",
      "Processing Activation Image For Layer-3, 64\n",
      "Processing Layer-0\n",
      "Processing Layer-1\n",
      "Processing Layer-2\n",
      "Processing Layer-3\n",
      "Processing Activation Image For Layer-0, 32\n",
      "Processing Activation Image For Layer-1, 32\n",
      "Processing Activation Image For Layer-2, 64\n",
      "Processing Activation Image For Layer-3, 64\n",
      "Processing Layer-0\n",
      "Processing Layer-1\n",
      "Processing Layer-2\n",
      "Processing Layer-3\n",
      "Processing Activation Image For Layer-0, 32\n",
      "Processing Activation Image For Layer-1, 32\n",
      "Processing Activation Image For Layer-2, 64\n",
      "Processing Activation Image For Layer-3, 64\n",
      "Processing Layer-0\n",
      "Processing Layer-1\n",
      "Processing Layer-2\n",
      "Processing Layer-3\n",
      "Processing Activation Image For Layer-0, 32\n",
      "Processing Activation Image For Layer-1, 32\n",
      "Processing Activation Image For Layer-2, 64\n",
      "Processing Activation Image For Layer-3, 64\n",
      "Loading DQN model from checkpoint: checkpoints/dqn/mario_net_12.chkpt\n",
      "Loading model at checkpoints/dqn/mario_net_12.chkpt with exploration rate 0.11943293650685695\n",
      "Model Loaded, Evaluating model ready for CNN Feature Visualisation\n",
      "Sequential(\n",
      "  (0): Conv2d(4, 32, kernel_size=(5, 5), stride=(2, 2), padding=(1, 1))\n",
      "  (1): ReLU()\n",
      "  (2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (3): ReLU()\n",
      "  (4): MaxPool2d(kernel_size=3, stride=3, padding=0, dilation=1, ceil_mode=False)\n",
      "  (5): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (6): ReLU()\n",
      "  (7): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (8): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (9): Flatten(start_dim=1, end_dim=-1)\n",
      "  (10): Linear(in_features=1024, out_features=128, bias=True)\n",
      "  (11): ReLU()\n",
      "  (12): Linear(in_features=128, out_features=3, bias=True)\n",
      ")\n",
      "Processing Layer-0\n",
      "Processing Layer-1\n",
      "Processing Layer-2\n",
      "Processing Layer-3\n",
      "Processing Activation Image For Layer-0, 32\n",
      "Processing Activation Image For Layer-1, 32\n",
      "Processing Activation Image For Layer-2, 64\n",
      "Processing Activation Image For Layer-3, 64\n",
      "Processing Layer-0\n",
      "Processing Layer-1\n",
      "Processing Layer-2\n",
      "Processing Layer-3\n",
      "Processing Activation Image For Layer-0, 32\n",
      "Processing Activation Image For Layer-1, 32\n",
      "Processing Activation Image For Layer-2, 64\n",
      "Processing Activation Image For Layer-3, 64\n",
      "Processing Layer-0\n",
      "Processing Layer-1\n",
      "Processing Layer-2\n",
      "Processing Layer-3\n",
      "Processing Activation Image For Layer-0, 32\n",
      "Processing Activation Image For Layer-1, 32\n",
      "Processing Activation Image For Layer-2, 64\n",
      "Processing Activation Image For Layer-3, 64\n",
      "Processing Layer-0\n",
      "Processing Layer-1\n",
      "Processing Layer-2\n",
      "Processing Layer-3\n",
      "Processing Activation Image For Layer-0, 32\n",
      "Processing Activation Image For Layer-1, 32\n",
      "Processing Activation Image For Layer-2, 64\n",
      "Processing Activation Image For Layer-3, 64\n",
      "Processing Layer-0\n",
      "Processing Layer-1\n",
      "Processing Layer-2\n",
      "Processing Layer-3\n",
      "Processing Activation Image For Layer-0, 32\n",
      "Processing Activation Image For Layer-1, 32\n",
      "Processing Activation Image For Layer-2, 64\n",
      "Processing Activation Image For Layer-3, 64\n",
      "Processing Layer-0\n",
      "Processing Layer-1\n",
      "Processing Layer-2\n",
      "Interrupted optimization at step 215.\n"
     ]
    }
   ],
   "source": [
    "movement = [\n",
    "    ['right'],\n",
    "    ['right', 'A'],\n",
    "    ['NOOP']\n",
    "]\n",
    "env, env_name = create_env(movement=movement)\n",
    "env = SkipFrame(env, skip=4)\n",
    "env = ResizeObservation(env, shape=84) # image dim: [84, 84]\n",
    "env = GrayScaleObservation(env, keep_dim=False) # Grayscale images\n",
    "env = FrameStack(env, num_stack=4) # 4 frames at a time\n",
    "obs = (4, 84, 84)\n",
    "\n",
    "for model_chpt in ['12']:#['8', '12']:\n",
    "    model_path = f'checkpoints/dqn/mario_net_{model_chpt}.chkpt'\n",
    "    print(f\"Loading DQN model from checkpoint: {model_path}\")\n",
    "    model = Mario(state_dim=obs, action_dim=env.action_space.n, save_dir=\".\")\n",
    "    path = Path(model_path)\n",
    "    model.load(path)\n",
    "    print(\"Model Loaded, Evaluating model ready for CNN Feature Visualisation\")\n",
    "    model.net.eval()\n",
    "    model_cnn = model.net.online # online_features for new DQN algo (pending checkpoints)\n",
    "    print(model_cnn)\n",
    "\n",
    "    # Frames\n",
    "    input_img_path = 'input_frame_stacks'\n",
    "    frames = [\n",
    "        ('frame_42', \"Jumping Over Goomba\"),\n",
    "        ('frame_70', \"Jumping Over Pipe\"),\n",
    "        ('frame_108',\"Falling In-front Of Goomba (In Air)\"),\n",
    "        ('frame_111',\"Falling In-front Of Goomba\"),\n",
    "        ('frame_230',\"Stuck On Pipe\"),\n",
    "        ('frame_233',\"Stuck On Pipe (In Air)\")\n",
    "    ]\n",
    "\n",
    "    n_step=512\n",
    "    # visualise_cnn_layers\n",
    "    conv_layers = [('0', 32, 0), ('2', 32, 1), ('5', 64, 2), ('7', 64, 3)] # (layer_idx, features, conv_layer_idx)\n",
    "\n",
    "    # # visualise_cnn_layers\n",
    "    for file_id, desc in frames:\n",
    "        if file_id != \"NOISE\":\n",
    "            frame_path = os.path.join(input_img_path, f\"{file_id}.png\")\n",
    "        else:\n",
    "            frame_path = file_id\n",
    "        visualise_cnn_layer_neurons(frame_path, model_cnn, conv_layers, device, n_step, f\"CNN Extracted Features Visualisation\\nDQN Model (Checkpoint {model_chpt}) - Scenario: {desc}\", f\"out/DQN/{model_chpt}chpt_{n_step}-steps_{desc.replace(' ', '-')}\")\n",
    "\n",
    "env.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
