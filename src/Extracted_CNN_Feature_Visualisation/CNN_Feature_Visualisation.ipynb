{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e3e1f006-802d-4325-a74a-3c3f78e31c23",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "2fd2055f-ea63-42d7-8958-2c5b5fe72a6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Super Mario Bros env dependencies\n",
    "from nes_py.wrappers import JoypadSpace\n",
    "import gym_super_mario_bros\n",
    "from gym_super_mario_bros.actions import SIMPLE_MOVEMENT, RIGHT_ONLY\n",
    "import gym\n",
    "from gym.spaces import Box\n",
    "from gym.wrappers import FrameStack, GrayScaleObservation, TransformObservation\n",
    "\n",
    "# Torch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Stable Baselines\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv\n",
    "\n",
    "# Networks to Evaluate\n",
    "import sys, os\n",
    "def add_to_path(model_dir):\n",
    "    notebook_file = os.path.dirname(\"CNN_Feature_Visualisation.ipynb\")\n",
    "    path2add = os.path.normpath(os.path.abspath(os.path.join(notebook_file, os.path.pardir, model_dir)))\n",
    "    if (not (path2add in sys.path)):\n",
    "        sys.path.append(path2add)\n",
    "add_to_path('DQN')\n",
    "from Agent import MarioNet, Mario\n",
    "add_to_path('a2c/a2c')\n",
    "from model import ACNetwork\n",
    "\n",
    "# CNN Visualisation (Lucent)\n",
    "from lucent.optvis import render, param, transform, objectives\n",
    "from lucent.misc.io import show\n",
    "\n",
    "# Utilities\n",
    "add_to_path('a2c/utils')\n",
    "from wrappers import ResizeObservation, SkipFrame\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a1d831b",
   "metadata": {},
   "source": [
    "## Model and Env Prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e5c0cc1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_env(random = False, record = None):\n",
    "    env_name = \"SuperMarioBros\"\n",
    "    if random:\n",
    "        env_name += \"RandomStage\"\n",
    "    env_name += '-v3'\n",
    "    env = gym_super_mario_bros.make(env_name)\n",
    "    env = JoypadSpace(env, SIMPLE_MOVEMENT)\n",
    "    if record:\n",
    "        print(\"Setting up recorder\")\n",
    "        out_dir, max_length, model_name = record\n",
    "        assert os.path.isdir(out_dir)\n",
    "        env = DummyVecEnv([lambda: env])\n",
    "        from stable_baselines3.common.vec_env import VecVideoRecorder\n",
    "        env = VecVideoRecorder(env, video_folder=out_dir, record_video_trigger=lambda _: 0, video_length=max_length, name_prefix=f\"{model_name}\")\n",
    "    env.reset()\n",
    "    return env, env_name\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e790ccc2",
   "metadata": {},
   "source": [
    "## Setup Feature Visualisation Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f786b61e-e884-4882-8d74-ea339b216ecb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Code derived from: https://colab.research.google.com/github/greentfrapp/lucent-notebooks/blob/master/notebooks/feature_inversion.ipynb#scrollTo=d47pkOPKvNjs\n",
    "@objectives.wrap_objective()\n",
    "def dot_compare(layer, batch=1, cossim_pow=0):\n",
    "    def inner(T):\n",
    "        dot = (T(layer)[batch] * T(layer)[0]).sum()\n",
    "        mag = torch.sqrt(torch.sum(T(layer)[0]**2))\n",
    "        cossim = dot/(1e-6 + mag)\n",
    "        return -dot * cossim ** cossim_pow\n",
    "    return inner\n",
    "\n",
    "transforms = [\n",
    "    transform.pad(8, mode='constant', constant_value=.5),\n",
    "    transform.jitter(8),\n",
    "    transform.random_scale([0.9, 0.95, 1.05, 1.1] + [1]*4),\n",
    "    transform.random_rotate(list(range(-5, 5)) + [0]*5),\n",
    "    transform.jitter(2),\n",
    "]\n",
    "\n",
    "def get_param_f(img, device, param):\n",
    "    img = torch.tensor(np.transpose(img, [2, 0, 1])).to(device)\n",
    "    # Initialize parameterized input and stack with target image\n",
    "    # to be accessed in the objective function\n",
    "    params, image_f = param.image(img.shape[1], channels=img.shape[0])\n",
    "    def stacked_param_f():\n",
    "        return params, lambda: torch.stack([image_f()[0], img])\n",
    "\n",
    "    return stacked_param_f\n",
    "\n",
    "def feature_inversion(img, layer, model, n_steps=512, cossim_pow=0.0):  \n",
    "    obj = objectives.Objective.sum([\n",
    "    1.0 * dot_compare(layer, cossim_pow=cossim_pow),\n",
    "    objectives.blur_input_each_step(),\n",
    "    ])\n",
    "\n",
    "    param_f = get_param_f(img)\n",
    "    images = render.render_vis(model, obj, param_f, transforms=transforms, preprocess=False, thresholds=(n_steps,), show_image=False)\n",
    "    return images\n",
    "\n",
    "def visualise_cnn_layers(src_img_path, model, convergence_steps, out_img_path):\n",
    "    image = np.array(Image.open(src_img_path), np.float32)\n",
    "    if len(image.shape) == 2:\n",
    "        image = image.reshape((image.shape[0], image.shape[1], 1))\n",
    "\n",
    "    # Extract Conv Layers\n",
    "    layers = ['0', '2', '4']\n",
    "    images = []\n",
    "    for layer in layers:\n",
    "        print(layer)\n",
    "        images = images + feature_inversion(image, layer, model, n_steps=convergence_steps)\n",
    "        print()\n",
    "        print([len(img) for img in images])\n",
    "    images = [images[0][1]] + [cnn_act_img[0] for cnn_act_img in images]\n",
    "    _, axs = plt.subplots(1, len(images))\n",
    "    for ax, image in zip(axs, images):\n",
    "        ax.imshow(image)\n",
    "        ax.xaxis.set_visible(False)\n",
    "        ax.yaxis.set_visible(False)\n",
    "    axs[0].set_title(\"Input\")\n",
    "    plt.savefig(f\"{out_img_path}.png\")\n",
    "\n",
    "def visualise_cnn_layer_neurons(src_img_path, model, convergence_steps, out_img_path):\n",
    "    image = np.array(Image.open(src_img_path), np.float32)\n",
    "    if len(image.shape) == 2:\n",
    "        image = image.reshape((image.shape[0], image.shape[1], 1))\n",
    "    param_f = get_param_f(image)\n",
    "    \n",
    "    # Extract Conv Layers\n",
    "    # Extract Neurons (Feature depth)\n",
    "    layers = {}\n",
    "    images = []\n",
    "    for layer_id, neurons in layers.items():\n",
    "        for neuron in neurons:\n",
    "            obj = f\"{layer_id}:{neuron}\"\n",
    "            print(obj)\n",
    "            images.append(render.render_vis(model, obj, param_f, transforms=transforms, preprocess=False, thresholds=(convergence_steps,), show_image=False))\n",
    "            print()\n",
    "            print([len(img) for img in images])\n",
    "    images = [images[0][0][1]] + [[cnn_act_img[0] for cnn_act_img in layer_cnn_act_img] for layer_cnn_act_img in images]\n",
    "    for layer in range(len(images)):\n",
    "        layer_images = images[layer]\n",
    "        _, axs = plt.subplots(1, len(layer_images))\n",
    "        for ax, image in zip(axs, layer_images):\n",
    "            ax.imshow(image)\n",
    "            ax.xaxis.set_visible(False)\n",
    "            ax.yaxis.set_visible(False)\n",
    "        axs[0].set_title(\"Input\")\n",
    "        plt.savefig(f\"{out_img_path}_layer-{layer}.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b2f9589",
   "metadata": {},
   "source": [
    "## A2C\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "a236789e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting up recorder\n",
      "ACNetwork(\n",
      "  (conv): Sequential(\n",
      "    (0): Conv2d(4, 32, kernel_size=(5, 5), stride=(2, 2), padding=(1, 1))\n",
      "    (1): ReLU()\n",
      "    (2): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1))\n",
      "    (3): ReLU()\n",
      "    (4): MaxPool2d(kernel_size=3, stride=3, padding=0, dilation=1, ceil_mode=False)\n",
      "    (5): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1))\n",
      "    (6): ReLU()\n",
      "    (7): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1))\n",
      "    (8): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (actor): Sequential(\n",
      "    (0): Linear(in_features=1024, out_features=128, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=128, out_features=7, bias=True)\n",
      "  )\n",
      "  (critic): Sequential(\n",
      "    (0): Linear(in_features=1024, out_features=128, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=128, out_features=1, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "env, env_name = create_env(record=('out/video', 1000, 'A2C'))\n",
    "env = SkipFrame(env, skip=4)\n",
    "env = ResizeObservation(env, shape=84) # image dim: [84, 84]\n",
    "env = GrayScaleObservation(env, keep_dim=False) # Grayscale images\n",
    "env = FrameStack(env, num_stack=4) # 4 frames at a time\n",
    "obs = (4, 84, 84)\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = ACNetwork(obs, env.action_space.n)\n",
    "checkpoint = torch.load('checkpoints/a2c/a2c_rollout10_ep100k.pt', map_location=device)\n",
    "model.load_state_dict(checkpoint['model'])\n",
    "print(model)\n",
    "model = model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4318514",
   "metadata": {},
   "source": [
    "## DQN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "ad18a40e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting up recorder\n",
      "Loading model at checkpoints/dqn/mario_net_12.chkpt with exploration rate 0.11943293650685695\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Error(s) in loading state_dict for MarioNet:\n\tMissing key(s) in state_dict: \"online_features.0.weight\", \"online_features.0.bias\", \"online_features.2.weight\", \"online_features.2.bias\", \"online_features.4.weight\", \"online_features.4.bias\", \"online_features.6.weight\", \"online_features.6.bias\", \"online_td_est.0.weight\", \"online_td_est.0.bias\", \"online_td_est.2.weight\", \"online_td_est.2.bias\", \"online_aux.0.weight\", \"online_aux.0.bias\", \"online_aux.2.weight\", \"online_aux.2.bias\", \"target_features.0.weight\", \"target_features.0.bias\", \"target_features.2.weight\", \"target_features.2.bias\", \"target_features.4.weight\", \"target_features.4.bias\", \"target_features.6.weight\", \"target_features.6.bias\", \"target_td_est.0.weight\", \"target_td_est.0.bias\", \"target_td_est.2.weight\", \"target_td_est.2.bias\". \n\tUnexpected key(s) in state_dict: \"online.0.weight\", \"online.0.bias\", \"online.2.weight\", \"online.2.bias\", \"online.5.weight\", \"online.5.bias\", \"online.7.weight\", \"online.7.bias\", \"online.10.weight\", \"online.10.bias\", \"online.12.weight\", \"online.12.bias\", \"target.0.weight\", \"target.0.bias\", \"target.2.weight\", \"target.2.bias\", \"target.5.weight\", \"target.5.bias\", \"target.7.weight\", \"target.7.bias\", \"target.10.weight\", \"target.10.bias\", \"target.12.weight\", \"target.12.bias\". ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_22374/1556966466.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMario\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate_dim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mobs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction_dim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maction_space\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msave_dir\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\".\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0mpath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'checkpoints'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;34m'dqn/mario_net_12.chkpt'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnet\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/dev/uni/CM50270_CW2/src/DQN/Agent.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(self, load_path)\u001b[0m\n\u001b[1;32m    165\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    166\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Loading model at {load_path} with exploration rate {exploration_rate}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 167\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    168\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexploration_rate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mexploration_rate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    169\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/bin/anaconda3/envs/rl_ppo/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mload_state_dict\u001b[0;34m(self, state_dict, strict)\u001b[0m\n\u001b[1;32m   1222\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror_msgs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1223\u001b[0m             raise RuntimeError('Error(s) in loading state_dict for {}:\\n\\t{}'.format(\n\u001b[0;32m-> 1224\u001b[0;31m                                self.__class__.__name__, \"\\n\\t\".join(error_msgs)))\n\u001b[0m\u001b[1;32m   1225\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0m_IncompatibleKeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmissing_keys\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0munexpected_keys\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1226\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Error(s) in loading state_dict for MarioNet:\n\tMissing key(s) in state_dict: \"online_features.0.weight\", \"online_features.0.bias\", \"online_features.2.weight\", \"online_features.2.bias\", \"online_features.4.weight\", \"online_features.4.bias\", \"online_features.6.weight\", \"online_features.6.bias\", \"online_td_est.0.weight\", \"online_td_est.0.bias\", \"online_td_est.2.weight\", \"online_td_est.2.bias\", \"online_aux.0.weight\", \"online_aux.0.bias\", \"online_aux.2.weight\", \"online_aux.2.bias\", \"target_features.0.weight\", \"target_features.0.bias\", \"target_features.2.weight\", \"target_features.2.bias\", \"target_features.4.weight\", \"target_features.4.bias\", \"target_features.6.weight\", \"target_features.6.bias\", \"target_td_est.0.weight\", \"target_td_est.0.bias\", \"target_td_est.2.weight\", \"target_td_est.2.bias\". \n\tUnexpected key(s) in state_dict: \"online.0.weight\", \"online.0.bias\", \"online.2.weight\", \"online.2.bias\", \"online.5.weight\", \"online.5.bias\", \"online.7.weight\", \"online.7.bias\", \"online.10.weight\", \"online.10.bias\", \"online.12.weight\", \"online.12.bias\", \"target.0.weight\", \"target.0.bias\", \"target.2.weight\", \"target.2.bias\", \"target.5.weight\", \"target.5.bias\", \"target.7.weight\", \"target.7.bias\", \"target.10.weight\", \"target.10.bias\", \"target.12.weight\", \"target.12.bias\". "
     ]
    }
   ],
   "source": [
    "env, env_name = create_env(record=('out/video', 1000, 'DQN'))\n",
    "env = JoypadSpace(\n",
    "    env,\n",
    "    [['right'],\n",
    "    ['right', 'A']]\n",
    ")\n",
    "env = SkipFrame(env, skip=4)\n",
    "env = ResizeObservation(env, shape=84) # image dim: [84, 84]\n",
    "env = GrayScaleObservation(env, keep_dim=False) # Grayscale images\n",
    "env = FrameStack(env, num_stack=4) # 4 frames at a time\n",
    "obs = (4, 84, 84)\n",
    "\n",
    "# device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = Mario(state_dim=obs, action_dim=env.action_space.n, save_dir=\".\")\n",
    "path = Path('checkpoints') / 'dqn/mario_net_12.chkpt'\n",
    "model.load(path)\n",
    "model = model.net\n",
    "print(model)\n",
    "model.eval()\n",
    "# model.online_features.load_state_dict(checkpoint[])\n",
    "# model.load_state_dict(torch.load('checkpoints/dqn/mario_net_4.chkpt'))\n",
    "# # print(model)\n",
    "# print(model)\n",
    "# model = model.eval()\n",
    "# print(model)\n",
    "\n",
    "# visualise_cnn_layers"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
