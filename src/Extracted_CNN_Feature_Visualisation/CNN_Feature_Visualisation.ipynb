{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e3e1f006-802d-4325-a74a-3c3f78e31c23",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2fd2055f-ea63-42d7-8958-2c5b5fe72a6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/whiffingj/bin/anaconda3/envs/rl/lib/python3.7/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Can torch see a GPU via cuda? Yes\n"
     ]
    }
   ],
   "source": [
    "# Super Mario Bros env dependencies\n",
    "from nes_py.wrappers import JoypadSpace\n",
    "import gym_super_mario_bros\n",
    "from gym_super_mario_bros.actions import SIMPLE_MOVEMENT, RIGHT_ONLY\n",
    "import gym\n",
    "\n",
    "# Torch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Stable Baselines\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv\n",
    "\n",
    "# Networks to Evaluate\n",
    "from ..DQN.Agent import MarioNet\n",
    "from ..a2c.a2c.model import ACNetwork\n",
    "\n",
    "# CNN Visualisation (Lucent)\n",
    "from lucent.optvis import render, param, transform, objectives\n",
    "from lucent.misc.io import show\n",
    "\n",
    "# Utilities\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import os\n",
    "\n",
    "def create_env(random = False, record = None):\n",
    "    env_name = \"SuperMarioBros\"\n",
    "    if random:\n",
    "        env_name += \"RandomStage\"\n",
    "    env_name += '-v3'\n",
    "    env = gym_super_mario_bros.make(env_name)\n",
    "    env = JoypadSpace(env, SIMPLE_MOVEMENT)\n",
    "    if record:\n",
    "        print(\"Setting up recorder\")\n",
    "        out_dir, max_length, model_name = record\n",
    "        assert os.path.isdir(out_dir)\n",
    "        env = DummyVecEnv([lambda: env])\n",
    "        from stable_baselines3.common.vec_env import VecVideoRecorder\n",
    "        env = VecVideoRecorder(env, video_folder=out_dir, record_video_trigger=lambda _: 0, video_length=max_length, name_prefix=f\"{model_name}\")\n",
    "    return env, env_name"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a1d831b",
   "metadata": {},
   "source": [
    "## Model and Env Prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5c0cc1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_env(random = False, record = None):\n",
    "    env_name = \"SuperMarioBros\"\n",
    "    if random:\n",
    "        env_name += \"RandomStage\"\n",
    "    env_name += '-v3'\n",
    "    env = gym_super_mario_bros.make(env_name)\n",
    "    env = JoypadSpace(env, SIMPLE_MOVEMENT)\n",
    "    if record:\n",
    "        print(\"Setting up recorder\")\n",
    "        out_dir, max_length, model_name = record\n",
    "        assert os.path.isdir(out_dir)\n",
    "        env = DummyVecEnv([lambda: env])\n",
    "        from stable_baselines3.common.vec_env import VecVideoRecorder\n",
    "        env = VecVideoRecorder(env, video_folder=out_dir, record_video_trigger=lambda _: 0, video_length=max_length, name_prefix=f\"{model_name}\")\n",
    "    return env, env_name\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e790ccc2",
   "metadata": {},
   "source": [
    "## Setup Feature Visualisation Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f786b61e-e884-4882-8d74-ea339b216ecb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Code derived from: https://colab.research.google.com/github/greentfrapp/lucent-notebooks/blob/master/notebooks/feature_inversion.ipynb#scrollTo=d47pkOPKvNjs\n",
    "@objectives.wrap_objective()\n",
    "def dot_compare(layer, batch=1, cossim_pow=0):\n",
    "    def inner(T):\n",
    "        dot = (T(layer)[batch] * T(layer)[0]).sum()\n",
    "        mag = torch.sqrt(torch.sum(T(layer)[0]**2))\n",
    "        cossim = dot/(1e-6 + mag)\n",
    "        return -dot * cossim ** cossim_pow\n",
    "    return inner\n",
    "\n",
    "transforms = [\n",
    "    transform.pad(8, mode='constant', constant_value=.5),\n",
    "    transform.jitter(8),\n",
    "    transform.random_scale([0.9, 0.95, 1.05, 1.1] + [1]*4),\n",
    "    transform.random_rotate(list(range(-5, 5)) + [0]*5),\n",
    "    transform.jitter(2),\n",
    "]\n",
    "\n",
    "def get_param_f(img, device, param):\n",
    "    img = torch.tensor(np.transpose(img, [2, 0, 1])).to(device)\n",
    "    # Initialize parameterized input and stack with target image\n",
    "    # to be accessed in the objective function\n",
    "    params, image_f = param.image(img.shape[1], channels=img.shape[0])\n",
    "    def stacked_param_f():\n",
    "        return params, lambda: torch.stack([image_f()[0], img])\n",
    "\n",
    "    return stacked_param_f\n",
    "\n",
    "def feature_inversion(img, layer, model, n_steps=512, cossim_pow=0.0):  \n",
    "    obj = objectives.Objective.sum([\n",
    "    1.0 * dot_compare(layer, cossim_pow=cossim_pow),\n",
    "    objectives.blur_input_each_step(),\n",
    "    ])\n",
    "\n",
    "    param_f = get_param_f(img)\n",
    "    images = render.render_vis(model, obj, param_f, transforms=transforms, preprocess=False, thresholds=(n_steps,), show_image=False)\n",
    "    return images\n",
    "\n",
    "def visualise_cnn_layers(src_img_path, model, convergence_steps, out_img_path):\n",
    "    image = np.array(Image.open(src_img_path), np.float32)\n",
    "    if len(image.shape) == 2:\n",
    "        image = image.reshape((image.shape[0], image.shape[1], 1))\n",
    "\n",
    "    # Extract Conv Layers\n",
    "    layers = ['0', '2', '4']\n",
    "    images = []\n",
    "    for layer in layers:\n",
    "        print(layer)\n",
    "        images = images + feature_inversion(image, layer, model, n_steps=convergence_steps)\n",
    "        print()\n",
    "        print([len(img) for img in images])\n",
    "    images = [images[0][1]] + [cnn_act_img[0] for cnn_act_img in images]\n",
    "    _, axs = plt.subplots(1, len(images))\n",
    "    for ax, image in zip(axs, images):\n",
    "        ax.imshow(image)\n",
    "        ax.xaxis.set_visible(False)\n",
    "        ax.yaxis.set_visible(False)\n",
    "    axs[0].set_title(\"Input\")\n",
    "    plt.savefig(f\"{out_img_path}.png\")\n",
    "\n",
    "def visualise_cnn_layer_neurons(src_img_path, model, convergence_steps, out_img_path):\n",
    "    image = np.array(Image.open(src_img_path), np.float32)\n",
    "    if len(image.shape) == 2:\n",
    "        image = image.reshape((image.shape[0], image.shape[1], 1))\n",
    "    param_f = get_param_f(image)\n",
    "    \n",
    "    # Extract Conv Layers\n",
    "    # Extract Neurons (Feature depth)\n",
    "    layers = {}\n",
    "    images = []\n",
    "    for layer_id, neurons in layers.items():\n",
    "        for neuron in neurons:\n",
    "            obj = f\"{layer_id}:{neuron}\"\n",
    "            print(obj)\n",
    "            images.append(render.render_vis(model, obj, param_f, transforms=transforms, preprocess=False, thresholds=(convergence_steps,), show_image=False))\n",
    "            print()\n",
    "            print([len(img) for img in images])\n",
    "    images = [images[0][0][1]] + [[cnn_act_img[0] for cnn_act_img in layer_cnn_act_img] for layer_cnn_act_img in images]\n",
    "    for layer in range(len(images)):\n",
    "        layer_images = images[layer]\n",
    "        _, axs = plt.subplots(1, len(layer_images))\n",
    "        for ax, image in zip(axs, layer_images):\n",
    "            ax.imshow(image)\n",
    "            ax.xaxis.set_visible(False)\n",
    "            ax.yaxis.set_visible(False)\n",
    "        axs[0].set_title(\"Input\")\n",
    "        plt.savefig(f\"{out_img_path}_layer-{layer}.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b2f9589",
   "metadata": {},
   "source": [
    "## A2C\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a236789e",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = load_model(args.model_path)\n",
    "model = model.policy\n",
    "model = model.features_extractor\n",
    "model = model.cnn\n",
    "print(model)\n",
    "model = model.to(device).eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4318514",
   "metadata": {},
   "source": [
    "## DQN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad18a40e",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = load_model(args.model_path)\n",
    "model = model.policy\n",
    "model = model.features_extractor\n",
    "model = model.cnn\n",
    "print(model)\n",
    "model = model.to(device).eval()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
